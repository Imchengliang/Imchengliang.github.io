<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>imchengliang</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-10-09T22:28:00.347Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Cheng Liang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Alpha-Beta Pruning</title>
    <link href="http://example.com/2022/10/09/alpha-beta-pruning/"/>
    <id>http://example.com/2022/10/09/alpha-beta-pruning/</id>
    <published>2022-10-09T14:47:23.000Z</published>
    <updated>2022-10-09T22:28:00.347Z</updated>
    
    <content type="html"><![CDATA[<p>Alpha-Beta pruning is a search algorithm used to reduce the number of nodes in a minimax search tree. When the algorithm evaluates that the subsequent moves of a strategy are worse than those of the previous strategy, it stops the subsequent development of the strategy. This algorithm achieves the same conclusion as the minimax algorithm, but prunes branches that don't affect the final decision.</p><h2 id="alpha-and-beta">Alpha and Beta</h2><p><span class="math inline">\(\alpha\)</span> is the value of the best (highest-value) choice we have found so far at any choice point along the path for MAX. And it's initialized as <span class="math inline">\(-\infty\)</span>.</p><p><span class="math inline">\(\beta\)</span> the value of the best (lowest-value) choice we have found so far at any choice point along the path for MIN. And it's initialized as <span class="math inline">\(\infty\)</span>.</p><h2 id="example-of-pruning">Example of Pruning</h2><p>The picture below is a case of a searching tree that has not been cut. In Alpha-Beta pruning, the first layer searches for the largest <span class="math inline">\(\alpha\)</span>, the second layer searches for the smallest <span class="math inline">\(\beta\)</span>, and so on for the following layers. The pruning is performed when the value of a node is larger (or equal) than <span class="math inline">\(\beta\)</span> of its parent node or smaller (or equal) than <span class="math inline">\(\alpha\)</span> of its parent node.</p><p><img src="/img/alpha_beta/1.jpg"></p><ol type="1"><li><p>Depth-first search starts at A and goes all the way to the last layer (nodes: -19, -2, 12). So the largest one 12 is set as the value for node E, and <span class="math inline">\(\alpha\)</span> in E is also updated as 12.</p></li><li><p>If the value in a child node is determined, the upper and lower bounds can be returned to the parent node. So <span class="math inline">\(\beta\)</span> of B can be updated as 12.</p></li><li><p>Keep searching from E to F, the second child node of F is 19, which is larger than <span class="math inline">\(\beta\)</span> of B. So the searching on this node can stop. So the value of F is set to be 19 and the branch with node 18 is pruned. It's the same for G, its third node is 18, which is larger than <span class="math inline">\(\beta\)</span> of B, then G is set to be 18.</p></li><li><p>When the child node values of B are all determined, B choose the smallest one (12) to be its value, and its <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span> can be returned to its parent node. So <span class="math inline">\(\alpha\)</span> of A is updated to be 12.</p></li><li><p>Depth-first search starts again from A to child nodes of H. The value of H is set to be its largest child node 12, and its <span class="math inline">\(\alpha\)</span> is also updated to be 12.</p></li><li><p>The upper bound and lower bound of H is returned to its parent node C. So <span class="math inline">\(\beta\)</span> of C is updated to be the value of H 12. In this case, <span class="math inline">\(\beta\)</span> of C is equal with <span class="math inline">\(\alpha\)</span> of A, so the searching on node C can stop and branches of I and J can be pruned.</p></li><li><p>Then the searching goes from A to the child nodes of K. The value of K and <span class="math inline">\(\alpha\)</span> of K are both updated to be the largest child node -6.</p></li><li><p>The value of K -6 is returned to node D and it updated to be <span class="math inline">\(\beta\)</span> of D. Because <span class="math inline">\(\alpha\)</span> of A is larger than <span class="math inline">\(\beta\)</span> of D, the searching on node D can stop and branches of L and M can be pruned.</p></li></ol><p><img src="/img/alpha_beta/2.png"></p><h2 id="alpha-beta-pruning-in-python">Alpha-Beta Pruning in Python</h2><figure class="highlight reasonml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">def max<span class="hljs-constructor">_value(<span class="hljs-params">self</span>, <span class="hljs-params">node</span>, <span class="hljs-params">alpha</span>,<span class="hljs-params">beta</span>)</span>:<br>    <span class="hljs-keyword">if</span>(self.is<span class="hljs-constructor">Terminal(<span class="hljs-params">node</span>)</span>){<br>        return node.get<span class="hljs-constructor">_value()</span><br>    }<br>clf = <span class="hljs-built_in">float</span>('-inf') <br>    <span class="hljs-keyword">for</span> chld <span class="hljs-keyword">in</span> node.children:<br>        clf = max(clf,min<span class="hljs-constructor">_value(<span class="hljs-params">chld</span>, <span class="hljs-params">alpha</span>, <span class="hljs-params">beta</span>)</span>)<br>        <span class="hljs-keyword">if</span> clf &gt;= beta:<br>            return clf<br>        alpha = max(alpha, clf)<br>    node.<span class="hljs-keyword">val</span> = clf<br>    return clf<br></code></pre></td></tr></tbody></table></figure><figure class="highlight reasonml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">def min<span class="hljs-constructor">_value(<span class="hljs-params">self</span>, <span class="hljs-params">node</span>, <span class="hljs-params">alpha</span>, <span class="hljs-params">beta</span>)</span>:<br>    <span class="hljs-keyword">if</span>(self.is<span class="hljs-constructor">Terminal(<span class="hljs-params">node</span>)</span>){<br>        return node.get<span class="hljs-constructor">_value()</span><br>    }<br>clf = <span class="hljs-built_in">float</span>('inf')<br>    <span class="hljs-keyword">for</span> chld <span class="hljs-keyword">in</span> node.children:<br>        chld = min(clf,max<span class="hljs-constructor">_value(<span class="hljs-params">chld</span>, <span class="hljs-params">alpha</span>, <span class="hljs-params">beta</span>)</span>)<br>        <span class="hljs-keyword">if</span> clf &lt;= alpha:<br>            return clf<br>        beta = min(beta, clf)<br>    node.<span class="hljs-keyword">val</span> = clf<br>    return clf<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Alpha-Beta pruning is a search algorithm used to reduce the number of nodes in a minimax search tree. When the algorithm evaluates that t</summary>
      
    
    
    
    <category term="Data Science" scheme="http://example.com/categories/Data-Science/"/>
    
    
    <category term="Artificial Intelligence" scheme="http://example.com/tags/Artificial-Intelligence/"/>
    
  </entry>
  
  <entry>
    <title>Breadth-First Search and Depth-First Search</title>
    <link href="http://example.com/2022/09/11/DFS-BFS/"/>
    <id>http://example.com/2022/09/11/DFS-BFS/</id>
    <published>2022-09-11T13:45:14.000Z</published>
    <updated>2022-10-09T22:30:04.079Z</updated>
    
    <content type="html"><![CDATA[<p>Breadth-First Search (BFS) and Depth-First Search (DFS) are two ways of traversing all vertices in a graph. These two basic search algorithms are introduced below.</p><h2 id="breadth-first-search">Breadth-First Search</h2><p><b>• Idea behind BFS</b></p><p>BFS is a traversal strategy for connected graphs, which is similar to the level-order traversal algorithm of binary trees. Its basic idea is: first visit the starting vertex <span class="math inline">\(v\)</span>, then start from <span class="math inline">\(v\)</span>, and visit each unvisited adjacent vertex of <span class="math inline">\(v\)</span> in turn, <span class="math inline">\(w_{1} ,w_{2},w_{3}....w_{n}\)</span>, and then visit all the unvisited adjacent vertices of <span class="math inline">\(w_{1} ,w_{2},w_{3}....w_{n}\)</span> in turn, after this, start from these visited vertices, and visit all of their unvisited adjacent vertices. And so on until all vertices on the graph have been visited.</p><p><b>• Algorithm steps for BFS</b></p><ol type="1"><li><p>Create an empty array and an empty queue, which are used to determine whether the location has been visited and enqueue unvisited nodes, respectively</p></li><li><p>Add the starting node to queue and set the position of this node on array as 1</p></li><li><p>Pop the first node on the queue out after getting its value, and determine whether the node is the target arrival point</p></li><li><p>If it is the target point, return the result (it usually is the shortest time or the shortest path)</p></li><li><p>If it is not the target point, continue to visit its adjacent nodes, enqueue the adjacent nodes that can be reached, and update the array</p></li><li><p>Repeat step 3, 4, 5 when the queue is not empty</p></li></ol><p><b>• Example of BFS</b></p><p>The following picture is the search tree of BFS for N-Queen Problem. The number near the node represents the appearance order of each node, <span class="math inline">\(X\)</span> represents the node that queen would be killed, and the last node is the goal state (it might change for different problem). <img src="/img/bfs_dfs/BFS.png"></p><h2 id="depth-first-search">Depth-First Search</h2><p><b>• Idea behind DFS</b></p><p>DFS is an algorithm for traversing or searching a tree or graph, which is based on backtracking method. It traverses the nodes of the tree along the depth of the tree, searching for branches of the tree as deep as possible. When the edges of the node <span class="math inline">\(v\)</span> have been explored or the node does not meet the conditions during the search, the search will backtrack to the starting node of the edge where the node <span class="math inline">\(v\)</span> is found. The whole process is repeated until all nodes are visited.</p><p><b>• Algorithm steps for DFS</b></p><ol type="1"><li><p>Create an empty stack to save unvisited nodes and an empty list to save visited nodes</p></li><li><p>Add the starting node and adjacent nodes stack and list in turn</p></li><li><p>Pop out the last node into the stack and get the node's neighbors from the graph</p></li><li><p>If the adjacent nodes are not on the list, add these nodes to stack and list</p></li><li><p>Output the node that is popped out</p></li><li><p>Repeat the step 3, 4, 5 until the stack is empty</p></li></ol><p><b>• Example of DFS</b></p><p>The following picture is the search tree of DFS for N-Queen Problem. The number near the node represents the appearance order of each node, <span class="math inline">\(X\)</span> represents the node that queen would be killed, and the last node is the goal state (it might change for different problem). <img src="/img/bfs_dfs/DFS.png"></p><h2 id="difference-between-bfs-and-dfs">Difference Between BFS and DFS</h2><table><colgroup><col style="width: 26%"><col style="width: 31%"><col style="width: 42%"></colgroup><thead><tr class="header"><th></th><th>BFS</th><th>DFS</th></tr></thead><tbody><tr class="odd"><td>Access Method</td><td>Accessing nodes layer by layer on the graph</td><td>Accessing nodes based on depth, visiting a node until reaching its last leaf or no unexplored node on the branch</td></tr><tr class="even"><td>When to visit next layer</td><td>Before visiting the child nodes, all nodes on the same layer has to be explored</td><td>Once an unexplored node is found, going the newly found node instead of keeping on exploring the current node</td></tr><tr class="odd"><td>Data Structure</td><td>Using queue to store unexplored nodes</td><td>Using stack to store unexplored nodes</td></tr><tr class="even"><td>Characteristic</td><td>Slow and require more memory</td><td>Fast and need less memory</td></tr><tr class="odd"><td>Application</td><td>Finding all connected components in a graph, searching the shortest path between two nodes, finding all nodes within a connected component, testing the biquality of a graph</td><td>Topological sorting, searching connected components, solving puzzles like mazes, finding joints (tangent vertices) of graphs</td></tr></tbody></table><h2 id="bfs-and-dfs-in-python">BFS and DFS in Python</h2><figure class="highlight crmsh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">class Graph:<br>    def __init__(self, *args, **kwargs):<br>        self.neighbors = {}<br>        self.visited = {}<br><br>    def add_nodes(self, nodes):<br>        for <span class="hljs-keyword">node</span> <span class="hljs-title">in</span> nodes: <br>            self.add_node(<span class="hljs-keyword">node</span><span class="hljs-title">)</span><br><span class="hljs-title">    </span><br><span class="hljs-title">    def</span> add_node(self, <span class="hljs-keyword">node</span><span class="hljs-title">):</span><br><span class="hljs-title">        print</span>('self.nodes()',self.nodes())<br>        if <span class="hljs-keyword">node</span> <span class="hljs-title">not</span> <span class="hljs-keyword">in</span> self.nodes():<br>            self.neighbors[<span class="hljs-keyword">node</span><span class="hljs-title">] = [node</span>]<br><br>    def add_edge(self, edge):<br>        u, v = edge<br>        if u not <span class="hljs-keyword">in</span> self.neighbors[v] <span class="hljs-keyword">and</span> v not <span class="hljs-keyword">in</span> self.neighbors[u]:<br>            self.neighbors[u].append(v)<br>            if u != v:  <br>                self.neighbors[v].append(u)<br>    <br>    def nodes(self):<br>        return self.neighbors.keys()<br>   <br>    <span class="hljs-comment"># DFS with recursion</span><br>    def depth_first_search(self, root = None):<br>        <span class="hljs-keyword">order</span> <span class="hljs-title">= []</span><br><span class="hljs-title">      </span><br><span class="hljs-title">        def</span> dfs(<span class="hljs-keyword">node</span><span class="hljs-title">):</span><br><span class="hljs-title">            self</span>.visited[<span class="hljs-keyword">node</span><span class="hljs-title">] = True</span><br>            order.append(<span class="hljs-keyword">node</span><span class="hljs-title">)</span><br><span class="hljs-title">            for</span> n <span class="hljs-keyword">in</span> self.neighbors[<span class="hljs-keyword">node</span><span class="hljs-title">]:</span><br><span class="hljs-title">                if</span> n not <span class="hljs-keyword">in</span> self.visited:<br>                    dfs(n) <br>        if root:<br>            dfs(root)  <span class="hljs-comment"># dfs(0)</span><br>        <span class="hljs-comment"># print('visited', self.visited) </span><br>        for <span class="hljs-keyword">node</span> <span class="hljs-title">in</span> self.nodes():<br>            if <span class="hljs-keyword">node</span> <span class="hljs-title">not</span> <span class="hljs-keyword">in</span> self.visited:<br>                dfs(<span class="hljs-keyword">node</span><span class="hljs-title">)</span><br><span class="hljs-title">        self</span>.visited = {}<br>        print(order)<br>        <span class="hljs-comment"># print('final visited', self.visited)</span><br>        return <span class="hljs-keyword">order</span><br>    <br>    <span class="hljs-title"># DFS</span> without recursion<br>    def depth_first_search_2(self, <span class="hljs-attr">root=</span>None):<br>        stack = []<br>        <span class="hljs-keyword">order</span> <span class="hljs-title">= []</span><br><span class="hljs-title">        def</span> dfs():<br>            while stack:<br>                print('stack',stack)<br>                <span class="hljs-keyword">node</span> <span class="hljs-title">= stack</span>[-<span class="hljs-number">1</span>] <span class="hljs-comment"># 栈顶元素</span><br>                for n <span class="hljs-keyword">in</span> self.neighbors[<span class="hljs-keyword">node</span><span class="hljs-title">]:</span><br><span class="hljs-title">                    if</span> n not <span class="hljs-keyword">in</span> self.visited:<br>                        order.append(n)<br>                        stack.append(n)<br>                        <span class="hljs-comment"># stack.append(self.neighbors[n])</span><br>                        self.visited[n] = <span class="hljs-literal">True</span><br>                        print('self.visited', self.visited)<br>                        break<br>                else: <br>                    stack.pop() <br>                <br>        if root:<br>            stack.append(root)<br>            order.append(root)<br>            self.visited[root] = <span class="hljs-literal">True</span><br>            dfs()    <br>        for <span class="hljs-keyword">node</span> <span class="hljs-title">in</span> self.nodes():<br>            if <span class="hljs-keyword">node</span> <span class="hljs-title">not</span> <span class="hljs-keyword">in</span> self.visited:<br>                stack.append(<span class="hljs-keyword">node</span><span class="hljs-title">)</span><br><span class="hljs-title">                order</span>.append(<span class="hljs-keyword">node</span><span class="hljs-title">)</span><br><span class="hljs-title">                self</span>.visited[<span class="hljs-keyword">node</span><span class="hljs-title">] = True</span>    <br>                dfs()       <br>        <span class="hljs-comment"># self.visited = {}  </span><br>        print(order)<br>        return <span class="hljs-keyword">order</span><br>    <br>    <span class="hljs-title"># BFS</span><br>    def breadth_first_search(self,<span class="hljs-attr">root=</span>None):<br>        que = []<br>        <span class="hljs-keyword">order</span> <span class="hljs-title">= []</span><br><span class="hljs-title">        def</span> bfs():<br>            while len(que) &gt; <span class="hljs-number">0</span>: <br>                <span class="hljs-keyword">node</span> <span class="hljs-title">= que</span>.pop(<span class="hljs-number">0</span>) <br>                self.visited[<span class="hljs-keyword">node</span><span class="hljs-title">] = True</span><br>                for n <span class="hljs-keyword">in</span> self.neighbors[<span class="hljs-keyword">node</span><span class="hljs-title">]:</span><br><span class="hljs-title">                    print</span>('self.visited', self.visited)<br>                    print('que', que)<br>                    if n not <span class="hljs-keyword">in</span> self.visited <span class="hljs-keyword">and</span> n not <span class="hljs-keyword">in</span> que:<br>                        que.append(n)<br>                        order.append(n)<br>        if root:<br>            que.append(root)<br>            order.append(root)<br>            bfs()<br>            <br>        for <span class="hljs-keyword">node</span> <span class="hljs-title">in</span> self.nodes():<br>            if not <span class="hljs-keyword">node</span> <span class="hljs-title">in</span> self.visited:<br>                que.append(<span class="hljs-keyword">node</span><span class="hljs-title">)</span><br><span class="hljs-title">                order</span>.append(<span class="hljs-keyword">node</span><span class="hljs-title">)</span><br><span class="hljs-title">                </span><br><span class="hljs-title">        self</span>.visited = {}<br>        print(order)<br>        return <span class="hljs-keyword">order</span>        <span class="hljs-title"></span><br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Breadth-First Search (BFS) and Depth-First Search (DFS) are two ways of traversing all vertices in a graph. These two basic search algori</summary>
      
    
    
    
    <category term="Data Science" scheme="http://example.com/categories/Data-Science/"/>
    
    
    <category term="Artificial Intelligence" scheme="http://example.com/tags/Artificial-Intelligence/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression</title>
    <link href="http://example.com/2022/08/13/linear-regression/"/>
    <id>http://example.com/2022/08/13/linear-regression/</id>
    <published>2022-08-13T07:21:06.000Z</published>
    <updated>2022-09-11T13:46:52.608Z</updated>
    
    <content type="html"><![CDATA[<p>Linear regression assumes that features and outcomes satisfy a linear relationship. The expressive ability of linear relationship is strong. The influence of each feature on the result can be reflected by its parameter.</p><p>And each feature variable can be mapped to a function first, and then participate in the linear calculation. In this way, a nonlinear relationship between features and results can be expressed.</p><p>The linear model can be expressed as <span class="math inline">\(f(X)=W^{T}X+b\)</span></p><p><span class="math inline">\(b\)</span> (bias parameter): compensates for the difference between the mean of the target values and the weighted mean of the basis function values</p><p>The goal is that the predicted value from model can be infinitely close to the true value: <span class="math inline">\(f(x_{i}) \approx y_{i}\)</span></p><p>The reason for we want infinitely close but not exactly the same is that we can only learn part of the data sampled from all the data of a certain type of event, the sampled data cannot cover all the possibilities of the event, so we can only learn the overall law in the end.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize_params</span>(<span class="hljs-params">dims</span>):</span><br>    w = np.zeros((dims, <span class="hljs-number">1</span>))<br>    b = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">return</span> w, b <br></code></pre></td></tr></tbody></table></figure><p>Here we use MSE to measure the difference between <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(y\)</span>: <span class="math inline">\(\mathrm{MSE}=\frac{1}{n} \sum_{i=1}^{n}\left(f(x_{i})-y_{i}\right)^{2}=\frac{1}{n} \sum_{i=1}^{n}\left(\hat{y}_{i}-y_{i}\right)^{2}\)</span></p><p>The difference can be positive or negative so using square can erase the effect of positive and negative. After the square is used, the error out of -1 and 1 will be enlarged, and the error between -1 and 1 will be reduced at the same time. And it's unable to handle when measures vary widely across dimensions, so we have to normalize the data before modeling.</p><p>Derivation Process of Parameter Estimation in Linear Regression Model:</p><p><span class="math inline">\(\left(w^{*}, b^{*}\right) =\underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(f\left(x_{i}\right)-y_{i}\right)^{2} =\underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}\)</span></p><p><span class="math inline">\(\frac{\partial E_{(w, b)}}{\partial w}=2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right), \quad\)</span> <span class="math inline">\(\frac{\partial E_{(w, b)}}{\partial b}=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right)\)</span></p><p><span class="math inline">\(w=\frac{\sum_{i=1}^{m} y_{i}\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}}, \quad\)</span> <span class="math inline">\(b=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\)</span></p><p>In matrix:</p><p><span class="math inline">\(w^{\prime} = 2X^{T}(Y-Xw) = 2X^{T}Y-2X^{T}Xw = 0\)</span></p><p><span class="math inline">\(2X^{T}Y = 2X^{T}Xw \rightarrow \hat{w} = (X^{T}X)^{-1}X^{T}Y\)</span></p><p><span class="math inline">\(X^{T}X\)</span> usually isn't a full-rank matrix in reality so we need to apply regularization on it</p><p><span class="math inline">\(\hat{w} = (X^{T}X+ \lambda I)^{-1}X^{T}Y\)</span></p><figure class="highlight reasonml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">def linear<span class="hljs-constructor">_loss(X, <span class="hljs-params">y</span>, <span class="hljs-params">w</span>, <span class="hljs-params">b</span>)</span>:<br>    num_train = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">X</span>.</span></span>shape<span class="hljs-literal">[<span class="hljs-number">0</span>]</span><br>    num_feature = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">X</span>.</span></span>shape<span class="hljs-literal">[<span class="hljs-number">1</span>]</span><br>    # model <span class="hljs-keyword">function</span><br>    y_hat = np.dot(X, w) + b<br>    # loss <span class="hljs-keyword">function</span><br>    loss = np.sum((y_hat-y)**<span class="hljs-number">2</span>)<span class="hljs-operator"> / </span>num_train<br>    # partial derivatives <span class="hljs-keyword">of</span> parameters<br>    dw = np.dot(X.T, (y_hat-y))<span class="hljs-operator"> / </span>num_train<br>    db = np.sum((y_hat-y))<span class="hljs-operator"> / </span>num_train<br>    return y_hat, loss, dw, db<br></code></pre></td></tr></tbody></table></figure><p>Update parameters based on gradient descent: multiple iterations are required to converge to the global minimum and a proper learning rate is necessary, but under this method, <span class="math inline">\((X^{T}X)^{-1}\)</span> isn't required to be calculated.</p><figure class="highlight ruby"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">linear_train</span><span class="hljs-params">(X, y, learning_rate, epochs)</span></span>:<br>    w, b = initialize_params(X.shape[<span class="hljs-number">1</span>])<br>    loss_list = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, epochs):<br>        <span class="hljs-comment"># calculate the current estimation, loss, and partial derivatives</span><br>        y_hat, loss, dw, db = linear_loss(X, y, w ,b)<br>        loss_list.append(loss)<br>        <span class="hljs-comment"># update parameters based on gradient descent</span><br>        w += -learning_rate * dw<br>        b += -learning_rate * db<br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10000</span> == <span class="hljs-number">0</span>:<br>            print(<span class="hljs-string">'epoch %d loss %f'</span> % (i, loss))<br><br>        <span class="hljs-comment"># save parameters</span><br>        params = {<span class="hljs-string">'w'</span><span class="hljs-symbol">:w</span>, <span class="hljs-string">'b'</span><span class="hljs-symbol">:b</span>}<br>        <span class="hljs-comment"># save gradient</span><br>        grads = {<span class="hljs-string">'dw'</span><span class="hljs-symbol">:dw</span>, <span class="hljs-string">'db'</span><span class="hljs-symbol">:db</span>}<br><br>    <span class="hljs-keyword">return</span> loss_list, loss, params, grads<br></code></pre></td></tr></tbody></table></figure><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-function">def <span class="hljs-title">predict</span>(<span class="hljs-params">X, <span class="hljs-keyword">params</span></span>): </span><br><span class="hljs-function">    w</span> = <span class="hljs-keyword">params</span>[<span class="hljs-string">'w'</span>]<br>    b = <span class="hljs-keyword">params</span>[<span class="hljs-string">'b'</span>]<br>    y_pred = np.dot(X, w) + b <br>    <span class="hljs-keyword">return</span> y_pred<br></code></pre></td></tr></tbody></table></figure><figure class="highlight nix"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs nix">from sklearn.datasets <span class="hljs-built_in">import</span> load_diabetes <br>from sklearn.utils <span class="hljs-built_in">import</span> shuffle<br><br><span class="hljs-attr">diabetes</span> = load_diabetes() <br><span class="hljs-attr">data</span> = diabetes.data <br><span class="hljs-attr">target</span> = diabetes.target<br><br><span class="hljs-comment"># shuffle the data</span><br>X, <span class="hljs-attr">y</span> = shuffle(data, target, <span class="hljs-attr">random_state=13)</span><br><span class="hljs-attr">X</span> = X.astype(np.float32)<br><br><span class="hljs-comment"># divide the train and test data set</span><br><span class="hljs-attr">offset</span> = int(X.shape[<span class="hljs-number">0</span>] * <span class="hljs-number">0.9</span>)<br>X_train, <span class="hljs-attr">y_train</span> = X[:offset], y[:offset]<br>X_test, <span class="hljs-attr">y_test</span> = X[offset:], y[offset:]<br><span class="hljs-attr">y_train</span> = y_train.reshape((-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br><span class="hljs-attr">y_test</span> = y_test.reshape((-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br></code></pre></td></tr></tbody></table></figure><figure class="highlight livecodeserver"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-comment"># visualize the prediction and real test result</span><br>import matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>f = X_test.dot(<span class="hljs-built_in">params</span>[<span class="hljs-string">'w'</span>]) + <span class="hljs-built_in">params</span>[<span class="hljs-string">'b'</span>]<br>plt.scatter(range(X_test.shape[<span class="hljs-number">0</span>]), y_test) <br>plt.plot(f, color = <span class="hljs-string">'darkorange'</span>) <br>plt.xlabel(<span class="hljs-string">'X'</span>)<br>plt.ylabel(<span class="hljs-string">'y'</span>)<br>plt.show()<br></code></pre></td></tr></tbody></table></figure><p><img src="/img/linear_regression/output.png"></p><figure class="highlight less"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs less"># <span class="hljs-selector-tag">plot</span> <span class="hljs-selector-tag">the</span> <span class="hljs-selector-tag">loss</span> <span class="hljs-selector-tag">during</span> <span class="hljs-selector-tag">training</span><br><span class="hljs-selector-tag">plt</span><span class="hljs-selector-class">.plot</span>(loss_list, color = <span class="hljs-string">'blue'</span>) <br><span class="hljs-selector-tag">plt</span><span class="hljs-selector-class">.xlabel</span>(<span class="hljs-string">'epochs'</span>) <br><span class="hljs-selector-tag">plt</span><span class="hljs-selector-class">.ylabel</span>(<span class="hljs-string">'loss'</span>)<br><span class="hljs-selector-tag">plt</span><span class="hljs-selector-class">.show</span>()<br></code></pre></td></tr></tbody></table></figure><p><img src="/img/linear_regression/output1.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Linear regression assumes that features and outcomes satisfy a linear relationship. The expressive ability of linear relationship is stro</summary>
      
    
    
    
    <category term="Data Science" scheme="http://example.com/categories/Data-Science/"/>
    
    
    <category term="Machine Learning" scheme="http://example.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Computation of Eigenvalues and Eigenvectors</title>
    <link href="http://example.com/2022/07/12/comp-eigen/"/>
    <id>http://example.com/2022/07/12/comp-eigen/</id>
    <published>2022-07-12T21:52:20.000Z</published>
    <updated>2022-07-13T22:06:07.281Z</updated>
    
    <content type="html"><![CDATA[<p>For a small matrix <span class="math inline">\(A\)</span>, we usually compute its eigenvalues through <span class="math inline">\(det(A-\lambda I)\)</span>. But in reality, this computation is too expensive for most matrices. And in many cases, we only need the largest and/or smallest eigenvalues (or just a few eigenvalues) so it might be overkill to always calculate all eigenvalues.</p><p>*Singular value decomposition is linked to eigenvalues and eigenvectors, so methods for eigenvalues and eigenvectors are also methods for SVD.</p><h2 id="iterative-methods">Iterative methods</h2><p>There are iterative methods for linear equation systems, nonlinear equations, eigenvalues / eigenvectors, optimization. And for eigenvalues/eigenvectors the only option in practice are iterative methods:</p><ol type="1"><li><p>A process that generates a sequence of approximations: <span class="math inline">\(v^{(1)}, v^{(2)}, v^{(3)}, \cdots\)</span></p></li><li><p>If the sequence converges the values gets closer and closer to the real value (otherwise it diverges)</p></li><li><p>Each step is based on the values in previous step</p></li><li><p>Typically an initial value (guess) is needed to start the process</p></li><li><p>The iterations are stopped when the values are good enough</p></li></ol><h2 id="the-power-method">The power method</h2><p>The steps of the power method are shown as below:</p><ol type="1"><li><p>Start with <span class="math inline">\(v_{0}=(1, 1, \cdots)^{\top}\)</span>, <span class="math inline">\(v_{t+1}=Av_{t}\)</span> and normalize <span class="math inline">\(v_{t+1}\)</span></p></li><li><p>Continue the iteration of (1) until the elements in <span class="math inline">\(v\)</span> don't change anymore</p></li><li><p>Normalize <span class="math inline">\(v_u\)</span> we find above with 2-norm to get the eigenvector, and compute the corresponding largest eigenvalue <span class="math inline">\(\lambda = \frac{v_{u}^{\top}(Av_u)}{v_{u}^{\top}v_u}=v_{u}^{\top}Av_u\)</span> (Note, if <span class="math inline">\(v\)</span> is normalized, <span class="math inline">\(v^{\top}v=1\)</span>)</p></li></ol><p>The mathematics theory behind this method:</p><ol type="1"><li><p>Initial a guess <span class="math inline">\(x^{(0)}\)</span>, if the matrix <span class="math inline">\(A\)</span> is diagonizable, then it has <span class="math inline">\(n\)</span> linearly independent eigenvectors: <span class="math inline">\(v_{1}, v_{2}, \cdots, v_{n}\)</span></p></li><li><p><span class="math inline">\(x^{(0)}\)</span> can be expressed as <span class="math inline">\(x^{(0)}=c_{1}v_{1}+c_{2}v_{2}+ \cdots +c_{n}v_{n}\)</span></p></li><li><p>Multiply by <span class="math inline">\(A\Rightarrow x^{(0)} = A x^{(0)} =A\left[c_{1} v_{1}+c_{2} v_{2}+\cdots+c_{n} V_{n}\right]\)</span><br><span class="math inline">\(\quad \quad \quad \quad \quad \quad \quad \quad \enspace=c_{1} A v_{1}+c_{2} A v_{2}+\cdots+c_{n} A v_{n}\)</span><br><span class="math inline">\(\quad \quad \quad \quad \quad \quad \quad \quad \enspace=c_{1} \lambda_{1} v_{1}+c_{2} \lambda_{2} v_{2}+\cdots+c_{n} \lambda _{n} V_{n}\)</span><br>Normalize the new <span class="math inline">\(x^{(0)} = \frac{x^{(0)}}{\parallel x^{(0)} \parallel _2}\)</span></p></li><li><p>Multiply by <span class="math inline">\(A\)</span> again <span class="math inline">\(\Rightarrow x^{(0)} = AA x^{(0)} =c_{1} \lambda_{1} A v_{1}+c_{2} \lambda_{2} A v_{2}+\cdots+c_{n} \lambda _{n} A V_{n}\)</span><br><span class="math inline">\(\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad =c_{1} {\lambda_{1}}^{2} v_{1}+c_{2} {\lambda_{2}}^{2} v_{2}+\cdots+c_{n} {\lambda _{n}}^{2} V_{n}\)</span><br><span class="math inline">\(\quad \quad \quad \quad \quad \quad \quad \quad \Rightarrow x^{(0)} = A^{k}x^{(0)}=c_{1} {\lambda_{1}}^{k} v_{1}+c_{2} {\lambda_{2}}^{k} v_{2}+\cdots+c_{n} {\lambda _{n}}^{k} V_{n}\)</span><br>Normalize the new $x^{(0)} again</p></li><li><p>Assume <span class="math inline">\(\lambda _{1}\)</span> is the largest in magnitude<br><span class="math inline">\(A^{k}x^{(0)}={\lambda_{1}}^{k}(c_{1}v_{1}+c_{2}(\frac{\lambda _{2}}{\lambda _{1}})^{k}v_{2}+ \cdots + c_{n}(\frac{\lambda _{n}}{\lambda _{1}})^{k} v_{n})\)</span><br><span class="math inline">\(\qquad \quad \approx{\lambda _{1}}^{k}c_{1}v_{1}\)</span>, <span class="math inline">\(\quad\)</span> when <span class="math inline">\(k\rightarrow \infty\)</span><br>Then we get a multiple of the dominant eigenvector.</p></li></ol><p>The convegence rate of the power method depends on the ratio of <span class="math inline">\(\lambda _{n}\)</span> and <span class="math inline">\(\lambda _{1}\)</span> (<span class="math inline">\(n=2,3,\cdots\)</span>). The convegence would be faster as <span class="math inline">\(\frac{\lambda _{n}}{\lambda _{1}}\)</span> to be smaller.</p><p>To find the smallest eigenvalue and corresponding eigenvector, we can perform the same iteration on <span class="math inline">\(A^{-1}\)</span>, called inverse power iteration.</p><p>By using shift <span class="math inline">\(\mu\)</span> and use the power method on the eigenvalue problem, we can find other eigenvalues than the largest: <span class="math inline">\((A-\mu I)v = (\lambda - \mu)v\)</span> In the case of <span class="math inline">\(\lambda _{1} = \mu\)</span>, the smallest eigenvalue becomes the largest in the shifted problem, and the Power method will find the smallest eigenvalue (to the initial problem). In principle we can find all eigenvalues by using shifts, but others methods are used in practice.</p><h2 id="the-qr-method">The QR-method</h2><p>The QR-method (or QR-iteration) is a method that can be used to find all eigenvalues at the same time. The idea is to in some way find transformations similar to <span class="math inline">\(A = QΛQ^{\top}\)</span> and then read off the eigenvalues on the diagonal of <span class="math inline">\(Λ\)</span>.</p><p>For the matrices that are not diagonalizable, we use a different similarity transform. If <span class="math inline">\(A = VBV^{\top}\)</span> for some nonsingular matrix <span class="math inline">\(V\)</span>, then <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are similar, so they have the same eigenvalues. If <span class="math inline">\(B\)</span> happens to be triangular, then we can read off the eigenvalues of <span class="math inline">\(A\)</span> from the diagonal of <span class="math inline">\(B\)</span>.</p><p>Schur decomposition (Schur form): If matrix <span class="math inline">\(A\)</span> real with real eigenvalues then there exist orthogonal matrix <span class="math inline">\(Q\)</span> such that <span class="math inline">\(A = QSQ^{-1} = QSQ^{\top}\)</span>, <span class="math inline">\(S\)</span> real and upper triangular. <span class="math inline">\(A\)</span> and <span class="math inline">\(S\)</span> are similar, so <span class="math inline">\(A\)</span>’s eigenvalues on the diagonal of <span class="math inline">\(S\)</span>. If <span class="math inline">\(A\)</span> real with complex eigenvalues then there exist orthogonal matrix <span class="math inline">\(Q\)</span> such that <span class="math inline">\(𝐴 = Q \tilde{S} Q^{-1} = Q \tilde{S} Q^{T}\)</span>, <span class="math inline">\(\tilde{S}\)</span> is quasi upper triangular.</p><ol type="1"><li><p>Start with <span class="math inline">\(A\)</span> doing QR-factorization: <span class="math inline">\(A=QR\)</span>, then <span class="math inline">\(A^{(1)}=RQ=Q^{\top}AQ\)</span></p></li><li><p>Next step: <span class="math inline">\(A^{(1)}=Q^{(1)}R\)</span>, form <span class="math inline">\(A^{(2)}=RQ^{(1)}={Q^{(1)}}^{\top}A^{(1)}Q^{(1)}\)</span>, and so forth</p></li></ol><p>If we look at <span class="math inline">\(A^{(3)}={Q^{(2)}}^{\top}{Q^{(1)}}^{\top}Q^{\top}AQQ^{(1)}Q^{(2)}\)</span>, <span class="math inline">\(\tilde{Q}\)</span> will converge to <span class="math inline">\(V\)</span> in the schur form contains eigenvectors, <span class="math inline">\(A^{(k)}\)</span> will converge to <span class="math inline">\(S\)</span> in schur form (triangular and eigenvalues on diagonal).</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;For a small matrix &lt;span class=&quot;math inline&quot;&gt;\(A\)&lt;/span&gt;, we usually compute its eigenvalues through &lt;span class=&quot;math inline&quot;&gt;\(det(A-\</summary>
      
    
    
    
    <category term="Mathematics" scheme="http://example.com/categories/Mathematics/"/>
    
    
    <category term="Linear Algebra" scheme="http://example.com/tags/Linear-Algebra/"/>
    
  </entry>
  
  <entry>
    <title>Principal Component Analysis (PCA)</title>
    <link href="http://example.com/2022/07/11/PCA/"/>
    <id>http://example.com/2022/07/11/PCA/</id>
    <published>2022-07-11T13:22:56.000Z</published>
    <updated>2022-07-12T21:41:17.901Z</updated>
    
    <content type="html"><![CDATA[<p>Principal Component Analysis is a technique for simplifying datasets. It is a linear transformation that transforms the data into a new coordinate system such that the first largest variance of any data projection is in the first coordinate (called the first principal component), and the second largest variance is in the second coordinate (the second principal component). ingredients), and so on. Principal component analysis is often used to reduce the dimensionality of a dataset while maintaining the features of the dataset that contribute the most to the variance. This is done by keeping low-order principal components and ignoring high-order principal components. Such lower-order components tend to retain the most important aspects of the data.</p><h2 id="covariance-matrix">Covariance matrix</h2><p>Algorithms like PCA depend heavily on the covariance. Correlation coefficient tells us how variables are related and it is the covariance normalized to range <span class="math inline">\([-1 \quad 1]\)</span>. The covariance matrix is an <span class="math inline">\(m*m\)</span>-matrix (<span class="math inline">\(m\)</span> is the number of variables) and it's symmetric as covariance between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> equals covariance between <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_1\)</span>. The diagonal entries are the variances (the covariance between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_1\)</span> is the variance of <span class="math inline">\(x_1\)</span>). There two methods for calculating the covariance matrix, and they're shown with the example below:</p><p><span class="math inline">\(x_{1}=\left(\begin{array}{l}2 \\ 3 \\ 4\end{array}\right), \quad \bar{x}_{1}=3, \quad x_{2}=\left(\begin{array}{l}3 \\ 1 \\ 2\end{array}\right), \quad \bar{x}_{2}=2\)</span></p><p>sample <span class="math inline">\(n=\)</span> number of elements in <span class="math inline">\(x_{n}=3, \quad\)</span> variable <span class="math inline">\(m=\)</span> number of <span class="math inline">\(x_{n}=2\)</span></p><p>Method 1:</p><p><span class="math inline">\(\sigma^{2}\left(x_{1}\right)=\frac{1}{n-1} \sum_{j=1}^{m}\left(x_{1 j}-\bar{x}_{1}\right)^{2}=\frac{1}{2}\left((2-3)^{2}+(3-3)^{2}+(4-3)^{2}\right)=1\)</span></p><p><span class="math inline">\(\sigma^{2}\left(x_{2}\right)=\frac{1}{n-1} \sum_{j=1}^{m}\left(x_{2 j}-\bar{x}_{2}\right)^{2}=\frac{1}{2}\left((3-2)^{2}+(1-2)^{2}+(2-2)^{2}\right)=1\)</span></p><p><span class="math inline">\(\sigma^{2}\left(x_{1}, x_{2}\right)=\frac{1}{n-1} \sum_{j=1}^{m}\left(x_{1 j}-\bar{x}_{1}\right)\left(x_{2 j}-\bar{x}_{2}\right)=-\frac{1}{2}\)</span></p><p><span class="math inline">\(\sigma^{2}\left(x_{2}, x_{1}\right)=\sigma^{2}\left(x_{1}, x_{2}\right)=-\frac{1}{2}\)</span></p><p><span class="math inline">\(C\left(x_{2}, x_{1}\right)=\left(\begin{array}{ll}\sigma^{2}\left(x_{1}\right) &amp; \sigma^{2}\left(x_{1}, x_{2}\right) \\ \sigma^{2}\left(x_{2}, x_{1}\right) &amp; \sigma^{2}\left(x_{2}\right)\end{array}\right)=\left(\begin{array}{cc}1 &amp; -1 / 2 \\ -1 / 2 &amp; 1\end{array}\right)\)</span></p><p>Method 2:</p><p>Let <span class="math inline">\(\bar{A}=\left(\begin{array}{l} x_{1}^{\top} \\ x_{2}^{\top}\end{array}\right)=\left(\begin{array}{ccc}2 &amp; 3 &amp; 4 \\ 3 &amp; 1 &amp; 2 \end{array}\right)\)</span></p><p>Apply scaling on matrix <span class="math inline">\(\bar{A}\)</span>: <span class="math inline">\(A=\left(\begin{array}{l} x_{1}^{\top}-\bar{x}_{1} \\ x_{2}^{\top}-\bar{x}_{2}\end{array}\right)=\left(\begin{array}{ccc}-1 &amp; 0 &amp; 1 \\ 1 &amp; -1 &amp; 0 \end{array}\right)\)</span></p><p><span class="math inline">\(C\left(x_{2}, x_{1}\right)=\frac{1}{n-1} A A^{\top}=\frac{1}{2}\left(\begin{array}{cc}2 &amp; -1 \\ -1 &amp; 2\end{array}\right)=\left(\begin{array}{cc}1 &amp; -1 / 2 \\ -1 / 2 &amp; 1\end{array}\right)\)</span></p><h2 id="svd-and-pca">SVD and PCA</h2><p>In SVD, <span class="math inline">\(A=U \Sigma V^{\top}\)</span>, then <span class="math inline">\(AV\)</span> or <span class="math inline">\(U \Sigma\)</span> represent data points's principal components. <span class="math inline">\(U=(u_{1}, \cdots, u_{n})\)</span> are the left singular vectors of <span class="math inline">\(A\)</span> (eigenvector of <span class="math inline">\(C\)</span>) that represent the direction of the largest variance of the data, which can also be view principal directions. We can get eigenvalues of <span class="math inline">\(C\)</span> from SVD of <span class="math inline">\(A\)</span>: <span class="math inline">\(\lambda _{i}=\frac{1}{n-1} \sigma_{i}^2\)</span>, which is also the magnitude of data points. Eigenvalues <span class="math inline">\(\lambda _{i}\)</span> represent the fraction of the total spread (variance) in the <span class="math inline">\(u_i\)</span>-direction.</p><p>Total variance = trace(<span class="math inline">\(C\)</span>) = the sum of eigenvalues of <span class="math inline">\(C\)</span> = the sum of diagonal elements of <span class="math inline">\(C\)</span>, and the number of each eigenvalue to be divided by total variance tells how many percents that each principal component explains the total variance. For example: there are 2 eigenvalues <span class="math inline">\(\lambda _{1}=28.9\)</span>, <span class="math inline">\(\lambda _{2}=0.1\)</span>, and trace(<span class="math inline">\(C\)</span>)=29, then <span class="math inline">\(\frac{\lambda _{1}}{trace(C)}= \frac{28.9}{29}=0.997\)</span>, so the first eigenvalue explains <span class="math inline">\(99\)</span>% of the total variance.</p><h2 id="pca-in-python">PCA in Python</h2><figure class="highlight stylus"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import numpy as np<br>Q = np<span class="hljs-selector-class">.array</span>(<span class="hljs-selector-attr">[[5,5,0,4]</span>, <span class="hljs-selector-attr">[1,1,5,0]</span>, <span class="hljs-selector-attr">[3,2,0,4]</span>, <span class="hljs-selector-attr">[3,5,0,5]</span>, <span class="hljs-selector-attr">[0,0,4,0]</span>])<br>A = Q-Q<span class="hljs-selector-class">.mean</span>(axis=<span class="hljs-number">0</span>, keepdims=True)<br>ATA = np<span class="hljs-selector-class">.dot</span>(A<span class="hljs-selector-class">.T</span>, A)<br>eig1 = np<span class="hljs-selector-class">.linalg</span><span class="hljs-selector-class">.eig</span>(ATA)<br>AAT = np<span class="hljs-selector-class">.dot</span>(A, A.T)<br>eig2 = np<span class="hljs-selector-class">.linalg</span><span class="hljs-selector-class">.eig</span>(AAT)<br>PCA = np<span class="hljs-selector-class">.dot</span>(A, eig1<span class="hljs-selector-attr">[1]</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(PCA, <span class="hljs-string">'\n'</span>)</span></span><br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Principal Component Analysis is a technique for simplifying datasets. It is a linear transformation that transforms the data into a new c</summary>
      
    
    
    
    <category term="Mathematics" scheme="http://example.com/categories/Mathematics/"/>
    
    
    <category term="Linear Algebra" scheme="http://example.com/tags/Linear-Algebra/"/>
    
  </entry>
  
  <entry>
    <title>Singular Value Decomposition (SVD) Part 2</title>
    <link href="http://example.com/2022/07/08/SVD2/"/>
    <id>http://example.com/2022/07/08/SVD2/</id>
    <published>2022-07-08T09:00:21.000Z</published>
    <updated>2022-07-12T21:40:12.352Z</updated>
    
    <content type="html"><![CDATA[<p>In <a href="https://imchengliang.top/2022/01/14/post-1/">Singular Value Decomposition (SVD) Part 1</a>, I introduce one way to calculate SVD. And in this post, I would talk about other ideas about solving SVD.</p><h2 id="matrix-a-is-symmetric">Matrix <span class="math inline">\(A\)</span> is symmetric</h2><p><span class="math display">\[\begin{aligned}A \cdot\left(q_{1}, \ldots q_{n}\right)&amp;=\left(\lambda_{1} q_{1} \ldots \lambda_{n} q_{n}\right) \\&amp;=\left(\begin{array}{lll}q_{1} &amp; \cdots &amp; q_{n}\end{array}\right) \cdot\left(\begin{array}{ccc}\lambda_{1} &amp; \cdots &amp; \cdots \\ \vdots &amp; \ddots &amp; \vdots \\ \cdots &amp; \lambda_{n} &amp; \cdots\end{array}\right)\end{aligned}\]</span> When the matrix <span class="math inline">\(A\)</span> is symmetric, the eigenvectors <span class="math inline">\(Q\)</span> of <span class="math inline">\(A\)</span> would be orthogonal. And <span class="math inline">\(Q^{-1} = Q^{\top}\)</span> for orthogonal matrix. Then we get the SVD as below: <span class="math display">\[\begin{aligned}AQ=Q \lambda  \quad \rightarrow  \quad A=Q \lambda Q^{-1}=Q \lambda Q^{\top}\end{aligned}\]</span></p><h2 id="matrix-a-isnt-symmetric">Matrix <span class="math inline">\(A\)</span> isn't symmetric</h2><p>In big data world, <span class="math inline">\(A\)</span> is a data matrix but it's not always a square matrix. Under this situation, we can't work directly with the eigenvectors of <span class="math inline">\(A\)</span>, <span class="math inline">\(A^{\top}A\)</span> and <span class="math inline">\(AA^{\top}\)</span> are required for SVD. Then we get "sort of" <span class="math inline">\(\lambda ^{2}\)</span>, which are two sets of eigenvectors:</p><ol type="1"><li><p>n right singular vectors, <span class="math inline">\(v_{1}, \cdots , v_{n}\)</span> orthogonal in <span class="math inline">\(R^n\)</span></p></li><li><p>m left singular vectors, <span class="math inline">\(u_{1}, \cdots , u_{n}\)</span> orthogonal in <span class="math inline">\(R^m\)</span></p></li></ol><p>If <span class="math inline">\(A=U \Sigma V^{\top}\)</span>, then <span class="math inline">\(A^{\top} A=\left(U \Sigma V^{\top}\right)^{\top}\left(U \Sigma V^{\top}\right)=V^{\top} \Sigma^{\top} U^{\top} U \Sigma V^{\top}=V \Sigma^{\top} \Sigma V^{\top} \rightarrow n \times n\)</span></p><p><span class="math inline">\(\Sigma^{\top} \Sigma=\left(\begin{array}{llll} \sigma_{1} &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \sigma_{n} \end{array}\right)\left(\begin{array}{lll} \sigma_{1} &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \sigma_{n} \end{array}\right)=\left(\begin{array}{lll} \sigma_{1}^{2} &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \sigma_{n}^{2} \end{array}\right)\)</span></p><p><span class="math inline">\(\rightarrow\left(\sigma_{1}{ }^{2} \cdots \sigma_{n}{ }^{2}\right)\)</span> are eigenvalues of <span class="math inline">\(A^{\top} A\)</span></p><p><span class="math inline">\(A A^{\top}=\left(U \Sigma V^{\top}\right)\left(U \Sigma V^{\top}\right)^{\top}=U \Sigma V^{\top} V^{\top} \Sigma^{\top} U^{\top}=U \Sigma \Sigma^{\top} U^{\top} \rightarrow m \times m\)</span></p><p><span class="math inline">\(\Sigma \Sigma^{\top}=\left(\begin{array}{llll} \sigma_{1} &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \sigma_{m} \end{array}\right)\left(\begin{array}{lll} \sigma_{1} &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \sigma_{m} \end{array}\right)=\left(\begin{array}{lll} \sigma_{1}^{2} &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \sigma_{m}^{2} \end{array}\right)\)</span></p><p><span class="math inline">\(\rightarrow\left(\sigma_{1}^{2} \cdots \sigma_{m}^{2}\right)\)</span> are eigenvalues of <span class="math inline">\(A A^{\top}\)</span></p><p>Then in <span class="math inline">\(A=U \Sigma V^{\top}\)</span>, <span class="math inline">\(V\)</span> is the eigenvector of <span class="math inline">\(A^{\top}A\)</span>, <span class="math inline">\(U\)</span> is the eigenvectors of <span class="math inline">\(AA^{\top}\)</span>, the diagonal elements in <span class="math inline">\(\Sigma\)</span> are the root of the first same n non-zero eigenvalues of <span class="math inline">\(A^{\top}A\)</span> and <span class="math inline">\(AA^{\top}\)</span>.</p><h2 id="example">Example</h2><p><span class="math inline">\(A=\left(\begin{array}{ll} 1 &amp; 0 \\ 4 &amp; 6 \\ 0 &amp; 1 \end{array}\right) \quad, \quad A^{\top} A=\left(\begin{array}{ll} 17 &amp; 24 \\ 24 &amp; 37 \end{array}\right) \quad, \quad A A^{\top}=\left(\begin{array}{ccc}1 &amp; 4 &amp; 0 \\ 4 &amp; 52 &amp; 6 \\ 0 &amp; 6 &amp; 1\end{array}\right)\)</span></p><p>For <span class="math inline">\(A^{\top} A\)</span></p><p>Eigenvalues: <span class="math inline">\(\lambda_{1}=53, \lambda_{2}=1\)</span></p><p>Eigenvectors: <span class="math inline">\(v_{1}=\left(\begin{array}{l}0.55 \\ 0.83\end{array}\right), \quad v_{2}=\left(\begin{array}{c}-0.83 \\ 0.55\end{array}\right)\)</span></p><p><span class="math inline">\(A^{\top} A \cdot\left(\begin{array}{cc}0.55 &amp; -0.83 \\ 0.83 &amp; 0.55\end{array}\right)=\left(\begin{array}{cc}0.55 &amp; -0.83 \\ 0.83 &amp; 0.55\end{array}\right) \cdot\left(\begin{array}{cc}53 &amp; 0 \\ 0 &amp; 1\end{array}\right)\)</span></p><p>For <span class="math inline">\(A A^{\top}\)</span></p><p>Eigenvalues: <span class="math inline">\(\lambda_{1}=53, \lambda_{2}=1, \lambda_{3}=0\)</span></p><p>Eigenvectors: <span class="math inline">\(u_{1}=\left(\begin{array}{l}0.08 \\ 0.99 \\ 0.11\end{array}\right), \quad u_{2}=\left(\begin{array}{c}-0.83 \\ 0 \\ 0.56\end{array}\right), \quad u_{3}=\left(\begin{array}{c}-0.55 \\ 0.14 \\ -0.82\end{array}\right)\)</span></p><p>Order the same eigenvalues of <span class="math inline">\(A^{\top}A\)</span> and <span class="math inline">\(AA^{\top}\)</span> from large to small: <span class="math inline">\(\sigma_{1}=\sqrt{53}, \quad \sigma_{2}=1\)</span></p><p><span class="math inline">\(A=U \Sigma V^{\top}=\left(\begin{array}{ccc} 0.08 &amp; -0.83 &amp; -0.55 \\ 0.99 &amp; 0 &amp; 0.14 \\ 0.11 &amp; 0.56 &amp; -0.82 \end{array}\right) \cdot\left(\begin{array}{cc} \sqrt{53} &amp; 0 \\ 0 &amp; 1 \\ 0 &amp; 0 \end{array}\right) \cdot\left(\begin{array}{cc} 0.55 &amp; 0.83 \\ -0.83 &amp; 0.35 \end{array}\right)\)</span></p><h2 id="svd-in-python">SVD in Python</h2><p>We can use numpy to construct a matrix in Python. The module linalg in numpy offers many functions of matrix operation</p><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">A</span> = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">0</span>]])<br><span class="hljs-comment"># VT is the Transpose of V, if we want the matrix V, use VT.T</span><br><span class="hljs-attribute">U</span>, Sigma, VT = np.linalg.svd(A, full_matrices=True)<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;In &lt;a href=&quot;https://imchengliang.top/2022/01/14/post-1/&quot;&gt;Singular Value Decomposition (SVD) Part 1&lt;/a&gt;, I introduce one way to calculate </summary>
      
    
    
    
    <category term="Mathematics" scheme="http://example.com/categories/Mathematics/"/>
    
    
    <category term="Linear Algebra" scheme="http://example.com/tags/Linear-Algebra/"/>
    
  </entry>
  
  <entry>
    <title>Tensor Operation</title>
    <link href="http://example.com/2022/07/06/Tensor-operation/"/>
    <id>http://example.com/2022/07/06/Tensor-operation/</id>
    <published>2022-07-06T09:50:08.000Z</published>
    <updated>2022-08-13T08:34:31.400Z</updated>
    
    <content type="html"><![CDATA[<p>In machine learning/big data we can think of a tensor as an nD-array. The picture below is an example of a rank 3 tensor with size (3,4,2). <img src="/img/tensor_operation/1.png"></p><h2 id="tensor-unfolding">Tensor unfolding</h2><p>Unfolding a tensor to a matrix (“matrization”) is a fundamental operation for most tensor methods and we can do it in different ways (use the tensor above as example).</p><p>Mode-1 unfolding: The column vectors of <span class="math inline">\(a_n\)</span> are column vectors of <span class="math inline">\(A_1\)</span> <span class="math display">\[A_{1}=\left(\begin{array}{cccccccc}0 &amp; 2 &amp; 4 &amp; 6 &amp; 1 &amp; 3 &amp; 5 &amp; 7 \\8 &amp; 10 &amp; 12 &amp; 14 &amp; 9 &amp; 11 &amp; 13 &amp; 15 \\16 &amp; 18 &amp; 20 &amp; 22 &amp; 17 &amp; 19 &amp; 21 &amp; 23\end{array}\right)\]</span></p><p>Mode-2 unfolding: The row vectors of <span class="math inline">\(a_n\)</span> are column vectors of <span class="math inline">\(A_2\)</span> <span class="math display">\[A_{2}=\left(\begin{array}{cccccc}0 &amp; 8 &amp; 16 &amp; 1 &amp; 9 &amp; 17 \\ 2 &amp; 10 &amp; 18 &amp; 3 &amp; 11 &amp; 19 \\ 4 &amp; 12 &amp; 20 &amp; 5 &amp; 13 &amp; 21 \\ 6 &amp; 14 &amp; 22 &amp; 7 &amp; 15 &amp; 23\end{array}\right)\]</span></p><p>Mode-3 unfolding: The mode-3 vectors of <span class="math inline">\(a_n\)</span> are columns vectors of <span class="math inline">\(A_3\)</span> <span class="math display">\[A_{3}=\left(\begin{array}{llllllllllll}0 &amp; 2 &amp; 4 &amp; 6 &amp; 8 &amp; 10 &amp; 12 &amp; 14 &amp; 16 &amp; 18 &amp; 20 &amp; 22 \\ 1 &amp; 3 &amp; 5 &amp; 7 &amp; 9 &amp; 11 &amp; 13 &amp; 15 &amp; 17 &amp; 19 &amp; 21 &amp; 23\end{array}\right)\]</span></p><h2 id="tensor-matrix-multiplication">Tensor-matrix multiplication</h2><p>1-mode multiplication: <span class="math inline">\(U \cdot a_n\)</span> (<span class="math inline">\(U\)</span> is the matrix that we need to multiply, and <span class="math inline">\(a_n\)</span> is the tensor we use)</p><ol type="1"><li><p>Mode-1 unfolding the tensor <span class="math inline">\(a_n \rightarrow A_1\)</span></p></li><li><p>Matrix-matrix multiplication <span class="math inline">\(U \cdot A_{1} = Y_{1}\)</span></p></li><li><p>Refold (fold the matrix back to a tensor) <span class="math inline">\(Y_{1} \rightarrow y_{1}\)</span></p></li></ol><p>Same principle for mode-2 multiplication and mode-3 multiplication etc.</p><h2 id="outer-product">Outer product</h2><p>Outer product between two vectors <span class="math inline">\(a^{1}, a^{2}\)</span> is <span class="math inline">\(a^{1} \cdot a^{2^{\top}}\)</span>, which is a 2D-matrix of rank=1 .</p><p>Outer product between three vectors <span class="math inline">\(a^{1}, a^{2}, a^{3}\)</span> is a tensor <span class="math inline">\(a_n\)</span> with three slices, and each slice is of rank=1. Each element in the tensor: <span class="math inline">\(a_n(i,j,k)\)</span> is defined by <span class="math inline">\({a^{1}}_{i} \cdot {a^{2}}_{j} \cdot {a^{3}}_{k}\)</span> (<span class="math inline">\(i\)</span> means i-th slice, <span class="math inline">\(j\)</span> means j-th row, <span class="math inline">\(k\)</span> means k-th column).</p><p>Frontal slices: in a <span class="math inline">\(3*3*3\)</span> tensor, the frontal slices is tensor[n, n, n], and n can be :, 0, 1, 2. : means choose all, 0 means to choose the first one, 1 means to choose the second one, and 2 means to choose the third one. The first n represents slice, the second n represents row, and the last n represents column.</p><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># Outer product</span><br><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">a1</span> = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<br><span class="hljs-attribute">a2</span> = np.array([<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>])<br><span class="hljs-attribute">a3</span> = np.array([<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>])<br><span class="hljs-attribute">t1</span> = np.outer(np.outer(a<span class="hljs-number">1</span>,a<span class="hljs-number">2</span>),a<span class="hljs-number">3</span>)<br><span class="hljs-attribute">t2</span> = t<span class="hljs-number">1</span>.reshape(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)<br><span class="hljs-comment"># Frontal slices</span><br><span class="hljs-attribute">t2</span>[:,:,<span class="hljs-number">0</span>] # first column of each slice<br><span class="hljs-attribute">t2</span>[<span class="hljs-number">0</span>,:,:] # first slice<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;In machine learning/big data we can think of a tensor as an nD-array. The picture below is an example of a rank 3 tensor with size (3,4,2</summary>
      
    
    
    
    
    <category term="Linear Algebra" scheme="http://example.com/tags/Linear-Algebra/"/>
    
  </entry>
  
  <entry>
    <title>QR Decomposition</title>
    <link href="http://example.com/2022/06/28/QR2/"/>
    <id>http://example.com/2022/06/28/QR2/</id>
    <published>2022-06-28T12:50:21.000Z</published>
    <updated>2022-07-06T09:42:12.662Z</updated>
    
    <content type="html"><![CDATA[<p>The QR (orthogonal triangular) decomposition is the most effective and widely used method to find all the eigenvalues of a general matrix. It decomposes the matrix into a normal orthogonal matrix <span class="math inline">\(Q\)</span> and an upper triangular matrix <span class="math inline">\(R\)</span>, so it is called the QR decomposition. There are two common methods of QR decomposition, one is based on Gram-Schmidt, and the other one is based on Householder reflectors.</p><h2 id="qr-decomposition-based-on-gram-schmidt">QR decomposition based on Gram-Schmidt</h2><p>Let say a matrix <span class="math inline">\(A=(a_{1} \quad a_{2} \quad a_{3})\)</span> for example, then <span class="math inline">\(Ax=(a_{1} \quad a_{2} \quad a_{3})x=b\)</span>. To find the <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span> for <span class="math inline">\(A\)</span>, we need to orthogonalize and normalize every vector in <span class="math inline">\(A\)</span> (* for the first vector <span class="math inline">\(a_1\)</span>, only normalization is required).</p><ol type="1"><li><p>Normalize <span class="math inline">\(a_1\)</span>: <span class="math inline">\(\quad g_{1}=\frac{a_{1}}{\parallel a_{1} \parallel}, \quad a_{1}=\left\|a_{1}\right\| g_{1}\)</span></p></li><li><p>Orthogonalize <span class="math inline">\(a_2\)</span>: <span class="math inline">\(\quad a_{2}^{\prime}=a_{2}-(a_{2}^{\top}g_{1})g_{1}\)</span></p></li><li><p>Normalize <span class="math inline">\(a_2\)</span>: <span class="math inline">\(\quad g_{2}=\frac{a_{2}^{\prime}}{\left\|a_{2}^{\prime}\right\|}, \quad a_{2}=\left(a_{2}^{\top} g_{1}\right) g_{1}+\left\|a_{2}^{\prime}\right\| g_{2}\)</span></p></li><li><p>Orthogonalise <span class="math inline">\(a_{3}\)</span>: <span class="math inline">\(\quad a_{3}^{\prime}=a_{3}-\left(a_{3}^{\top} g_{1}\right) g_{1}-\left(a_{3}^{\top} g_{2}\right) g_{2}\)</span></p></li><li><p>Normalize <span class="math inline">\(a_{3}\)</span>: <span class="math inline">\(\quad g_{3}=\frac{a_{3}{ }^{\prime}}{\left\|a_{3}\right\|}, \quad a_{3}=\left(a_{3}^{\top} g_{1}\right) g_{1}+\left(a_{3}^{\top} g_{2}\right) g_{2}+\left\|a_{3}^{\prime}\right\| g_{3}\)</span></p></li></ol><p>After the orthogonalization and normalization, we get <span class="math inline">\(Q=\left(\begin{array}{lll} g_{1} &amp; g_{2} &amp; g_{3} \end{array}\right)\)</span> and <span class="math inline">\(R=\left(\begin{array}{ccc} \left\|a_{1}\right\| &amp; a_{2}^{\top} g_{1} &amp; a_{3}^{\top} g_{1} \\ 0 &amp; \left\|a_{2}\right\| &amp; a_{3}^{\top} g_{2} \\ 0 &amp; 0 &amp; \left\|a_{3}\right\| \end{array}\right)\)</span></p><p>One problem with Gram-Schmidt is that <span class="math inline">\(g_{1}, g_{2}, g_{3}\)</span> won't be exactly orthogonal due to error in the computations (round-off).</p><p>And as <span class="math inline">\(g_3\)</span> depends on <span class="math inline">\(g_1\)</span> and <span class="math inline">\(g_2\)</span>, the problem tends to get worse and worse. The vectors get less and less orthogonal in practice. Therefore, it's better to use Householder reflectors when we apply QR decomposition.</p><h2 id="qr-decomposition-based-on-householder-reflectors">QR decomposition based on Householder reflectors</h2><p>Householder reflector: <span class="math inline">\(H=I-2uu^{\top}=I-\frac{2 v v^{\top}}{\|v\|_{2}{ }^{2}}, \quad H^{2}=I\)</span></p><p>For every reflector, it is orthogonal and doesn't change length <span class="math inline">\(\rightarrow H^{2}=H^{\top}H=I\)</span></p><p>Now consider <span class="math inline">\(A\)</span> as a <span class="math inline">\(4*3\)</span> matrix for example, then <span class="math inline">\(Q_{n}\)</span> <span class="math inline">\((n=1,2,3)\)</span>, all are square matrix with size of <span class="math inline">\(4*4\)</span>. We can find <span class="math inline">\(Q_{n}\)</span> one by one following the steps below.</p><p>Find <span class="math inline">\(Q_1\)</span> that makes <span class="math inline">\(Q_{1}A=\left(\begin{array}{ccc}x &amp; x &amp; x \\ 0 &amp; x &amp; x \\ 0 &amp; x &amp; x \\ 0 &amp; x &amp; x\end{array}\right)\)</span>:</p><p><span class="math inline">\(\quad v=a_{1}+\left\|a_{1}\right\|e_{1} \quad\)</span> (<span class="math inline">\(a_{1}\)</span> is the first column in <span class="math inline">\(A\)</span>), <span class="math inline">\(\quad Q_{1}=I - \frac{2 v v^{\top}}{\|v\|_{2}{ }^{2}}\)</span></p><p>Find <span class="math inline">\(Q_2\)</span> that makes <span class="math inline">\(Q_{2}(Q_{1}A)=\left(\begin{array}{ccc}x &amp; x &amp; x \\ 0 &amp; x &amp; x \\ 0 &amp; 0 &amp; x \\ 0 &amp; 0 &amp; x\end{array}\right)\)</span>:</p><p><span class="math inline">\(\quad v=a_{2}+\left\|a_{2}\right\|e_{1} \quad\)</span> (<span class="math inline">\(a_{2}\)</span> is last three elements of the second column in <span class="math inline">\(Q_{1}A\)</span>)</p><p><span class="math inline">\(\quad Q_{2}^{\prime}=I - \frac{2 v v^{\top}}{\|v\|_{2}{ }^{2}}\)</span>, <span class="math inline">\(\quad Q_{2}=\left(\begin{array}{cccc}1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; Q_{2_{1,1}}^{\prime} &amp; Q_{2_{1,2}}^{\prime} &amp; Q_{2_{1,3}}^{\prime} \\ 0 &amp; Q_{2_{2,1}}^{\prime} &amp; Q_{2_{2,2}}^{\prime} &amp; Q_{2_{2,3}}^{\prime} \\ 0 &amp; Q_{2_{3,1}}^{\prime} &amp; Q_{2_{3,2}}^{\prime} &amp; Q_{2_{3,3}}^{\prime} \end{array}\right)\)</span></p><p>Find <span class="math inline">\(Q_3\)</span> that makes <span class="math inline">\(Q_{3}(Q_{2}Q_{1}A)=\left(\begin{array}{ccc}x &amp; x &amp; x \\ 0 &amp; x &amp; x \\ 0 &amp; 0 &amp; x \\ 0 &amp; 0 &amp; 0\end{array}\right)\)</span>:</p><p><span class="math inline">\(\quad v=a_{3}+\left\|a_{3}\right\|e_{1} \quad\)</span> (<span class="math inline">\(a_{3}\)</span> is last two elements of the third column in <span class="math inline">\(Q_{2}(Q_{1}A)\)</span>)</p><p><span class="math inline">\(\quad Q_{3}^{\prime}=I - \frac{2 v v^{\top}}{\|v\|_{2}{ }^{2}}\)</span>, <span class="math inline">\(\quad Q_{3}=\left(\begin{array}{cccc}1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; Q_{3_{1,1}}^{\prime} &amp; Q_{3_{1,2}}^{\prime} \\ 0 &amp; 0 &amp; Q_{3_{2,1}}^{\prime} &amp; Q_{3_{2,2}}^{\prime} \end{array}\right)\)</span></p><p><span class="math inline">\(Q_{3} Q_{2} Q_{1} A=R \rightarrow A=Q_{1}^{\top} Q_{2}^{\top} Q_{3}^{\top} R=Q R\)</span>, and we get <span class="math inline">\(Q=Q_{1}^{\top} Q_{2}^{\top} Q_{3}^{\top}\)</span></p><h2 id="qr-decomposition-in-python">QR Decomposition in Python</h2><figure class="highlight maxima"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs maxima">import math<br>import argparse<br>import numpy as <span class="hljs-built_in">np</span><br>from typing import Union<br><br># QR decomposition based on Gram-Schmidt<br>def gram_schmidt(A):<br>    cols = A.shape[<span class="hljs-number">1</span>]<br>    Q = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">copy</span>(A)<br>    R = <span class="hljs-built_in">np</span>.zeros((cols, cols))<br>    <span class="hljs-keyword">for</span> <span class="hljs-built_in">col</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(cols):<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">col</span>):<br>            k =  <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>(a[:, <span class="hljs-built_in">col</span>] * Q[:, i]) / <span class="hljs-built_in">np</span>.<span class="hljs-built_in">sum</span>( <span class="hljs-built_in">np</span>.square(Q[:, i]) )<br>            Q[:, <span class="hljs-built_in">col</span>] -= k*Q[:, i]<br>        Q[:, <span class="hljs-built_in">col</span>] /= <span class="hljs-built_in">np</span>.linalg.norm(Q[:, <span class="hljs-built_in">col</span>])<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(cols):<br>            R[<span class="hljs-built_in">col</span>, i] = Q[:, <span class="hljs-built_in">col</span>].dot( A[:, i] )   <br>    <span class="hljs-built_in">return</span> Q, R  <br><br># QR decomposition based on Householder reflectors  <br>def householder(alpha: <span class="hljs-built_in">float</span>, x: <span class="hljs-built_in">np</span>.ndarray) -&gt; Union[<span class="hljs-built_in">np</span>.ndarray, int]:<br>    s = math.pow(<span class="hljs-built_in">np</span>.linalg.norm(x, ord=<span class="hljs-number">2</span>), <span class="hljs-number">2</span>)<br>    v = x<br>    <span class="hljs-keyword">if</span> s == <span class="hljs-number">0</span>:<br>        tau = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">else</span>:<br>        t = math.<span class="hljs-built_in">sqrt</span>(alpha * alpha + s)<br>        v_one = alpha - t <span class="hljs-keyword">if</span> alpha &lt;= <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> -s / (alpha + t)<br>        tau = <span class="hljs-number">2</span> * v_one * v_one / (s + v_one * v_one)<br>        v /= v_one<br>    <span class="hljs-built_in">return</span> v, tau<br><br>def qr_decomposition(A: <span class="hljs-built_in">np</span>.ndarray, m: int, n: int) -&gt; Union[<span class="hljs-built_in">np</span>.ndarray, <span class="hljs-built_in">np</span>.ndarray]:<br>    H = []<br>    R = A<br>    Q = A<br>    I = <span class="hljs-built_in">np</span>.eye(m, m)<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, n):<br>        # Apply Householder transformation.<br>        x = A[j + <span class="hljs-number">1</span>:m, j]<br>        v_householder, tau = householder(<span class="hljs-built_in">np</span>.linalg.norm(x), x)<br>        v = <span class="hljs-built_in">np</span>.zeros((<span class="hljs-number">1</span>, m))<br>        v[<span class="hljs-number">0</span>, j] = <span class="hljs-number">1</span><br>        v[<span class="hljs-number">0</span>, j + <span class="hljs-number">1</span>:m] = v_householder<br>        res = I - tau * v * <span class="hljs-built_in">np</span>.<span class="hljs-built_in">transpose</span>(v)<br>        R = <span class="hljs-built_in">np</span>.matmul(res, R)<br>        H.<span class="hljs-built_in">append</span>(res)<br>    <span class="hljs-built_in">return</span> Q, R<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;The QR (orthogonal triangular) decomposition is the most effective and widely used method to find all the eigenvalues of a general matrix</summary>
      
    
    
    
    <category term="Mathematics" scheme="http://example.com/categories/Mathematics/"/>
    
    
    <category term="Linear Algebra" scheme="http://example.com/tags/Linear-Algebra/"/>
    
  </entry>
  
  <entry>
    <title>The Least Squares Problem</title>
    <link href="http://example.com/2022/04/16/QR-decomposition/"/>
    <id>http://example.com/2022/04/16/QR-decomposition/</id>
    <published>2022-04-15T22:02:08.000Z</published>
    <updated>2022-07-06T09:40:58.938Z</updated>
    
    <content type="html"><![CDATA[<p>The Least Squares Problem is one of the most important problem in numerical approximation. The main idea of the least squares problem is to solve the unknown parameters , so that the sum of the squares of the difference between the predicted value and the observed value (i.e. the error, or residual) is minimized. In short, it can be interpreted as solving the equation of <span class="math inline">\(Ax=b\)</span>. In linear algebra, there are three common ways to solve the least squares problem, and their introduction and comparison are shown as below.</p><h2 id="the-normal-equations">The normal equations</h2><p>For an overdetermined systems (<span class="math inline">\(m*n\)</span>): <span class="math inline">\(Ax=b\)</span> such as: <span class="math display">\[\left(\begin{array}{ll}1 &amp; 4 \\1 &amp; 2 \\1 &amp; 3\end{array}\right)\left(\begin{array}{l}x_{1} \\x_{2}\end{array}\right)=\left(\begin{array}{l}3 \\3 \\2\end{array}\right) \Leftrightarrow\left(\begin{array}{l}1 \\1 \\1\end{array}\right) x_{1}+\left(\begin{array}{l}4 \\2 \\3\end{array}\right) x_{2}=\left(\begin{array}{l}3 \\3 \\2\end{array}\right)\]</span> We can't solve the system by <span class="math inline">\(a_{1}x_{1}+a_{2}x_{2}=b\)</span> because we can't reach <span class="math inline">\(b\)</span> through a linear combination of <span class="math inline">\(a_1\)</span> and <span class="math inline">\(a_2\)</span>. This method only works when <span class="math inline">\(b \in C(A)\)</span> (It's very unlikely when <span class="math inline">\(m&gt;n\)</span>).</p><p>Therefore, we find the vector <span class="math inline">\(p\)</span> closest to <span class="math inline">\(b\)</span> in the same plane of <span class="math inline">\(a_1\)</span> and <span class="math inline">\(a_2\)</span>, and let <span class="math inline">\(e=b-p\)</span> to be as small as possible and orthogonal to <span class="math inline">\(a_1\)</span> and <span class="math inline">\(a_2\)</span> (<span class="math inline">\(C(A)\)</span>).</p><p>Then find a solution <span class="math inline">\(\hat{x}\)</span> such that <span class="math inline">\(p=a_{1}\hat{x}_{1}+a_{2}\hat{x}_{2}\)</span>, and <span class="math inline">\(p\)</span> is the projection of <span class="math inline">\(b\)</span> onto <span class="math inline">\(C(A)\)</span>. <span class="math display">\[\begin{aligned}&amp;p=a_{1} \hat{x}_{1}+a_{2} \hat{x}_{2}=A \hat{x} \\&amp;e=b-p=b-A \hat{x} \\\end{aligned}\]</span> Orthogonality yield <span class="math display">\[\begin{aligned}&amp;\left\{\begin{array}{l}a_{1} \perp e \Leftrightarrow a_{1}^{T} e=0 \\a_{2} \perp e \Leftrightarrow a_{2}^{T} e=0\end{array} \Leftrightarrow A^{T} e=0 \Leftrightarrow A^{T}(b-A \hat{x})=0\right.\end{aligned}\]</span></p><p>The normal equations is <span class="math inline">\(A^{T}A\hat{x}=A^{T}b\)</span>. It's a least squares question and normally requires <span class="math inline">\(A^{T}A\)</span> to be non-singular. It can prove that <span class="math inline">\(rank(A)=rank(A^{T}A)\)</span>, so <span class="math inline">\(A\)</span> must have independent columns. The normal equations can be solved with Cholesky decomposition (<span class="math inline">\(A^{T}A\)</span> symm. pos. def.) but <span class="math inline">\(A^{T}A\)</span> is often ill-conditioned.</p><p>The normal equations requires less operation than other methods, but it won't work when <span class="math inline">\(A\)</span> is singular it also has a high condition number.</p><h2 id="qr-decomposition">QR decomposition</h2><p>QR decomposition <span class="math inline">\(A=QR\)</span> is one way to solve least squares problem. The solution is shown as below. <span class="math display">\[\begin{gathered}A^{T} A x=A^{T} b \Rightarrow(Q R)^{T} Q R x=(Q R)^{T} b \Rightarrow \\R^{T} Q^{T} Q R x=R^{T} Q^{T} b \Rightarrow R^{T} R x=R^{T} Q^{T} b \Rightarrow \\R x=Q^{T} b\end{gathered}\]</span> Solving <span class="math inline">\(R x=Q^{T} b\)</span> (with backward substitution) gives the least squares solution.</p><p>The merit of QR decomposition is that it has nothing to do with condition number. But the operation of QR decomposition is expensive and matrix <span class="math inline">\(A\)</span> has to be singular otherwise there will be no solution for it.</p><h2 id="persudo-inverse">Persudo-inverse</h2><p>When <span class="math inline">\(A\)</span> is a full-rank square matrix, there is a solution for <span class="math inline">\(Ax=b\)</span> that <span class="math inline">\(x=A^{-1}b\)</span>. But when <span class="math inline">\(A\)</span> isn't a full-rank square matrix, there is no solution for this equation. So we need to find the approximate solution <span class="math inline">\(x^{\prime}=\arg \min \|A x-b\|=A^{+} b\)</span>, and <span class="math inline">\(A^{+}\)</span> is a pseudo-inverse matrix.</p><p>Let the SVD of <span class="math inline">\(A\)</span> be <span class="math display">\[A=U\left(\begin{array}{ll}S &amp; 0 \\0 &amp; 0\end{array}\right) V^{T},\]</span> where <span class="math inline">\(U, V\)</span> are both orthogonal matrices, and <span class="math inline">\(S\)</span> is a diagonal matrix containing the (positive) singular values of <span class="math inline">\(A\)</span> on its diagonal. Then the pseudo-inverse of <span class="math inline">\(A\)</span> is the <span class="math inline">\(n \times m\)</span> matrix defined as <span class="math display">\[A^{+}=V\left(\begin{array}{cc}S^{-1} &amp; 0 \\0 &amp; 0\end{array}\right) U^{T} .\]</span> Note that <span class="math inline">\(A^{+}\)</span> has the same dimension as the transpose of <span class="math inline">\(A\)</span>.</p><p>If <span class="math inline">\(A\)</span> is square, invertible, then its inverse is <span class="math inline">\(A^{+}=A^{-1}\)</span>.</p><p>If <span class="math inline">\(A\)</span> is full column rank, meaning <span class="math inline">\(\operatorname{rank}(A)=n \leq m\)</span>, that is, <span class="math inline">\(A^{\top} A\)</span> is not singular, then <span class="math inline">\(A^{+}\)</span> is a left inverse of <span class="math inline">\(A\)</span>, in the sense that <span class="math inline">\(A^{+} A=I_{n}\)</span>. We have the closed-form expression <span class="math inline">\(A^{+}=\left(A^{\top} A\right)^{-1} A^{\top}\)</span></p><p>If <span class="math inline">\(A\)</span> is full row rank, meaning <span class="math inline">\(\operatorname{rank}(A)=m \leq n\)</span>, that is, <span class="math inline">\(A A^{\top}\)</span> is not singular, then <span class="math inline">\(A^{+}\)</span> is a right inverse of <span class="math inline">\(A\)</span>, in the sense that <span class="math inline">\(A A^{+}=I_{m}\)</span>. We have the closed-form expression <span class="math inline">\(A^{+}=A^{\top}\left(A A^{\top}\right)^{-1}\)</span></p><p>The solution to the least-squares problem <span class="math inline">\(\min _{x}\|A x-b\|_{2}\)</span> with minimum norm is <span class="math inline">\(x^{*}=A^{+} b\)</span>.</p><p>Persudo-inverse will work all the time but its operation is also quite expensive due to the singular value decomposition part.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;The Least Squares Problem is one of the most important problem in numerical approximation. The main idea of the least squares problem is </summary>
      
    
    
    
    <category term="Mathematics" scheme="http://example.com/categories/Mathematics/"/>
    
    
    <category term="Linear Algebra" scheme="http://example.com/tags/Linear-Algebra/"/>
    
  </entry>
  
  <entry>
    <title>Cholesky Decomposition</title>
    <link href="http://example.com/2022/04/15/cholesky/"/>
    <id>http://example.com/2022/04/15/cholesky/</id>
    <published>2022-04-15T14:52:50.000Z</published>
    <updated>2022-04-15T21:59:26.125Z</updated>
    
    <content type="html"><![CDATA[<p>Cholesky decomposition is to express a symmetric positive definite matrix as a decomposition of the product of a lower triangular matrix and its transpose. It requires that all eigenvalues of the matrix must be positive, so the diagonal elements of the decomposed lower triangle are also positive. The Cholesky decomposition, also known as the square root method, is a modification of the <a href="https://imchengliang.top/2022/04/14/LU/">LU decomposition</a> when matrix A is a symmetric positive definite matrix.</p><h2 id="ldlt-decomposition">(1) LDLT decomposition</h2><p>If <span class="math inline">\(A\)</span> is symmetric, we would expect some sort of symmetry in the LU decomposition.</p><p><span class="math inline">\(U \ne L^T\)</span> due to diagonals in <span class="math inline">\(U\)</span> and <span class="math inline">\(L\)</span> not the same. But if we take <span class="math inline">\(u_{ii}\)</span> out of <span class="math inline">\(U\)</span> and store in diagonal matrix <span class="math inline">\(D\)</span> we get <span class="math inline">\(U = DL^T\)</span>, and <span class="math inline">\(A=LDL^T\)</span> called the LDLT decomposition.</p><p>It's roughly half the cost of LU decomposition as we don’t need to compute the upper diagonal part of <span class="math inline">\(U\)</span>.</p><h2 id="cholesky-decomposition">(2) Cholesky decomposition</h2><p>When <span class="math inline">\(A\)</span> is symmetric, positive definite (<span class="math inline">\(d_{ii} &lt; 0\)</span> in <span class="math inline">\(D\)</span>), the LDLT decomposition becomes Cholesky decomposition (Cholesky is stable - no pivoting needed): <span class="math display">\[A=LD^{1/2}D^{1/2}L^T=GG^T\]</span></p><p>It’s possible to calculate Cholesky directly without forming <span class="math inline">\(L\)</span> and <span class="math inline">\(D\)</span>. Use <span class="math display">\[A=\left(\begin{array}{ccc}a_{11} &amp; \cdots &amp; a_{1 n} \\\vdots &amp; \ddots &amp; \vdots \\a_{n 1} &amp; \cdots &amp; a_{n n}\end{array}\right)=\left(\begin{array}{ccc}g_{11} &amp; \cdots &amp; 0 \\\vdots &amp; \ddots &amp; \vdots \\g_{n 1} &amp; \cdots &amp; g_{n n}\end{array}\right)\left(\begin{array}{ccc}g_{11} &amp; \cdots &amp; g_{n 1} \\\vdots &amp; \ddots &amp; \vdots \\0 &amp; \cdots &amp; g_{n n}\end{array}\right)\]</span></p><h2 id="how-to-determine-matrix-is-pos.-def">(3) How to determine matrix is pos. def</h2><p>Definition <span class="math inline">\(x^{T}Ax &gt; 0\)</span> is not very useful, <span class="math inline">\(\lambda &gt; 0\)</span> works but it's too expensive to compute eigenvalues.</p><p>If not:</p><ol type="1"><li><p>Check if symmetric</p></li><li><p>Check if all diagonal elements are positive (this is just a sign of pos. def.)</p></li><li><p>Try Cholesky, and if Cholesky fail, exit and perform standard LU</p></li></ol><h2 id="cholesky-decomposition-in-python">(4) Cholesky decomposition in Python</h2><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">from</span> scipy.linalg import ldl<br><span class="hljs-attribute">from</span> scipy.linalg import cholesky<br><span class="hljs-attribute">from</span> scipy.linalg import solve<br><span class="hljs-attribute">A</span>=np.array([[<span class="hljs-number">9</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">10</span>,<span class="hljs-number">7</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">7</span>,<span class="hljs-number">6</span>]]);<br><span class="hljs-comment"># LDLT-decomposition</span><br><span class="hljs-attribute">L</span>, D, P = ldl(A,lower=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># Cholesky-decomposition</span><br><span class="hljs-attribute">G</span> = cholesky(A, lower=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># <span class="hljs-doctag">Note:</span> scipy.solve can solve systems using</span><br><span class="hljs-comment"># Cholesky. Number of operations halved.</span><br><span class="hljs-comment"># assume_a='pos' =&gt; ldlt-solution</span><br><span class="hljs-attribute">b</span> = np.array([[<span class="hljs-number">8</span>],<span class="hljs-meta"> [-1], [-4]])</span><br><span class="hljs-meta">x = solve(A,b, assume_a='pos')</span><br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Cholesky decomposition is to express a symmetric positive definite matrix as a decomposition of the product of a lower triangular matrix </summary>
      
    
    
    
    <category term="Mathematics" scheme="http://example.com/categories/Mathematics/"/>
    
    
    <category term="Linear Algebra" scheme="http://example.com/tags/Linear-Algebra/"/>
    
  </entry>
  
  <entry>
    <title>LU Decomposition</title>
    <link href="http://example.com/2022/04/14/LU/"/>
    <id>http://example.com/2022/04/14/LU/</id>
    <published>2022-04-13T22:02:47.000Z</published>
    <updated>2022-04-15T21:49:39.343Z</updated>
    
    <content type="html"><![CDATA[<p>LU decomposition is a common method to solve polynomial <span class="math inline">\(Ax=b\)</span>, and this method is consist of three steps:</p><ol type="1"><li><p>LU factorization: Gaussian elimination on matrix <span class="math inline">\(A\)</span>, factorize <span class="math inline">\(A\)</span> to <span class="math inline">\(L,U\)</span> such that <span class="math inline">\(PA=LU\)</span></p></li><li><p>Forward substitution: Solve lower triangular system <span class="math inline">\(Ld = Pb\)</span></p></li><li><p>Backward substitution: Solve upper triangular system <span class="math inline">\(Ux = d\)</span></p></li></ol><p>In the following part, I would use the example of <span class="math inline">\(A\)</span> and <span class="math inline">\(b\)</span> to explain the calculation of LU decomposition. <span class="math display">\[A=\left(\begin{array}{ccc}1 &amp; 2 &amp; 3 \\2 &amp; 5 &amp; 10 \\3 &amp; 10 &amp; 10\end{array}\right), \quad b=\left(\begin{array}{c}3 \\7 \\13\end{array}\right)\]</span></p><h2 id="lu-factorization">(1) LU factorization</h2><p>In LU factorization, we need to factorize <span class="math inline">\(A\)</span> into <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span>, and let <span class="math inline">\(LU=PA\)</span> <span class="math display">\[L=\left(\begin{array}{ccc}a_1 &amp; 0 &amp; 0 \\a_2 &amp; a_3 &amp; 0 \\a_4 &amp; a_5 &amp; a_6\end{array}\right), \quad U=\left(\begin{array}{ccc}b_1 &amp; b_2 &amp; b_3 \\0 &amp; b_4 &amp; b_5 \\0 &amp; 0 &amp; b_6\end{array}\right)\]</span></p><p>Sometimes, we can get the right result through the decomposition of <span class="math inline">\(LU=A\)</span>, but this decomposition isn't stable that might cause a large error in the calculation. Therefore, we need to do the "pivoting" procedure (<span class="math inline">\(P\)</span>) during the factorization step. "pivoting" means that we should keep the value of each non-zero element of each row to be larger the other elements of the same column below this one, and we can achieve this goal by row exchange.</p><p><span class="math inline">\(U\)</span> is obtained from Gaussian elimination of <span class="math inline">\(A\)</span>, and <span class="math inline">\(L\)</span> is computed from <span class="math inline">\(U\)</span>. <span class="math inline">\(P\)</span> is the identity matrix at first, and if we exchange row during the Gaussian elimination, the same exchange need to be applied on <span class="math inline">\(P\)</span>. And <span class="math inline">\(A, P, L, U\)</span> all have the same size.</p><p>The calculation below is LU factorization example:</p><p><span class="math display">\[\begin{aligned}&amp;A=\left(\begin{array}{ccc}1 &amp; 2 &amp; 3 \\2 &amp; 5 &amp; 10 \\3 &amp; 10 &amp; 16\end{array}\right), \quad P=\left(\begin{array}{lll}1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1\end{array}\right) \\&amp;\stackrel{R_{1} \leftrightarrow R_{3}}{\longrightarrow}\left(\begin{array}{ccc}3 &amp; 10 &amp; 16 \\2 &amp; 5 &amp; 10 \\1 &amp; 2 &amp; 3\end{array}\right), \quad P=\left(\begin{array}{lll}0 &amp; 0 &amp; 1 \\0 &amp; 1 &amp; 0 \\1 &amp; 0 &amp; 0\end{array}\right) \\&amp;\stackrel{R_{2}-\frac{2}{3} R_{1}}{\longrightarrow}\left(\begin{array}{ccc}3 &amp; 10 &amp; 10 \\0 &amp; -\frac{5}{3} &amp; -\frac{2}{3} \\0 &amp; -\frac{4}{3} &amp; -\frac{1}{3}\end{array}\right), \quad P=\left(\begin{array}{lll}0 &amp; 0 &amp; 1 \\0 &amp; 1 &amp; 0 \\1 &amp; 0 &amp; 0\end{array}\right) \\&amp;\stackrel{R_{3}-\frac{1}{5} R_{1} R_{2}}{\longrightarrow}\left(\begin{array}{ccc}3 &amp; 10 &amp; 16 \\0 &amp; -\frac{5}{3} &amp; -\frac{2}{3} \\0 &amp; 0 &amp; -\frac{5}{5}\end{array}\right)=U, \quad P=\left(\begin{array}{lll}0 &amp; 0 &amp; 1 \\0 &amp; 1 &amp; 0 \\1 &amp; 0 &amp; 0\end{array}\right)\end{aligned}\]</span></p><p><span class="math display">\[\begin{aligned}L&amp;=\left(\left(\begin{array}{l}3 \\2 \\1\end{array}\right) / 3,\left(\begin{array}{c}0 \\-\frac{5}{3} \\-\frac{4}{3}\end{array}\right) / -\frac{5}{3},\left(\begin{array}{c}0 \\0 \\-\frac{9}{5}\end{array}\right) /-\frac{9}{5}\right) \\&amp;=\left(\begin{array}{ccc}1 &amp; 0 &amp; 0 \\\frac{2}{3} &amp; 1 &amp; 0 \\\frac{1}{3} &amp; \frac{4}{5} &amp; 1\end{array}\right)\end{aligned}\]</span></p><h2 id="forward-and-backward-substitution">(2) Forward and Backward substitution</h2><p>Forward and Backward substitution is used to calculate the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> in the following functions. <span class="math display">\[\begin{aligned}&amp;\left\{\begin{array} { l } { A x = b } \\{ L U = P A } \\{ U x = y }\end{array} \rightarrow \left\{\begin{array}{l}L y=P b \\U x=y\end{array}\right.\right.\\\end{aligned}\]</span> Forward substitution: <span class="math display">\[\begin{aligned}&amp;P b=\left(\begin{array}{lll}0 &amp; 0 &amp; 1 \\0 &amp; 1 &amp; 0 \\1 &amp; 0 &amp; 0\end{array}\right) \cdot\left(\begin{array}{l}3 \\7 \\13\end{array}\right)=\left(\begin{array}{c}13 \\7 \\3\end{array}\right)\\&amp;L y=\left(\begin{array}{ccc}1 &amp; 0 &amp; 0 \\\frac{2}{3} &amp; 1 &amp; 0 \\\frac{1}{3} &amp; \frac{4}{5} &amp; 1\end{array}\right) \cdot\left(\begin{array}{l}y_{1} \\y_{2} \\y_{3}\end{array}\right)=\left(\begin{array}{c}13 \\7 \\3\end{array}\right)\\&amp;\rightarrow\left\{\begin{array} { l } { y _ { 1 } = 1 3 } \\{ \frac { 2 } { 3 } y _ { 1 } + y _ { 2 } = 1 } \\{ \frac { 1 } { 3 } y _ { 1 } + \frac { 4 } { 5 } y _ { 2 } + y _ { 3 } = 3 }\end{array} \rightarrow \left\{\begin{array}{l}y_{1}=13 \\y_{2}=-\frac{5}{3} \\y_{3}=0\end{array} \quad \rightarrow y=\left(\begin{array}{c}13 \\-\frac{5}{3} \\0\end{array}\right)\right.\right.\\\end{aligned}\]</span> Backward substitution: <span class="math display">\[\begin{aligned}&amp;Ux=\left(\begin{array}{ccc}3 &amp; 10 &amp; 16 \\0 &amp; -\frac{5}{3} &amp; -\frac{2}{3} \\0 &amp; 0 &amp; -\frac{9}{5}\end{array}\right) \cdot\left(\begin{array}{l}x_{1} \\x_{2} \\x_{3}\end{array}\right)=\left(\begin{array}{c}13 \\-\frac{5}{3} \\0\end{array}\right)\\&amp;\rightarrow\left\{\begin{array} { c } { 3 x _ { 1 } + 1 0 x _ { 2 } + 1 6 x _ { 3 } = 1 3 } \\{ - \frac { 5 } { 3 } x _ { 2 } - \frac { 2 } { 3 } x _ { 3 } = - \frac { 5 } { 3 } } \\{ - \frac { 9 } { 5 } x _ { 3 } = 0 }\end{array} \rightarrow \left\{\begin{array}{l}x_{1}=1 \\x_{2}=1 \\x_{3}=0\end{array} \rightarrow x=\left(\begin{array}{l}1 \\1 \\0\end{array}\right)\right.\right.\end{aligned}\]</span></p><h2 id="lu-decomposition-in-python">(3) LU decomposition in Python</h2><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import numpy as np<br><span class="hljs-keyword">from</span> scipy.linalg import lu<br><span class="hljs-keyword">from</span> scipy.linalg import solve_triangular<br><span class="hljs-attribute">A</span>=np.array([[3 -1 2],[1 0 -1],[4 2 -3]])<br>P, L, U =lu(A)<br><span class="hljs-attribute">b</span>=np.array([[8],[-1],[-4]])<br>b = P.T @ b # Row changes <span class="hljs-keyword">in</span> b<br><span class="hljs-comment"># Forward and backward substitution</span><br><span class="hljs-attribute">d</span>=solve_triangular(L, b, <span class="hljs-attribute">lower</span>=<span class="hljs-literal">True</span>)<br><span class="hljs-attribute">x</span>=solve_triangular(U, y)<br><span class="hljs-comment"># Check if the result from LU-decomposition is the same as direct computation</span><br>x2 = np.linalg.solve(A,b) <br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;LU decomposition is a common method to solve polynomial &lt;span class=&quot;math inline&quot;&gt;\(Ax=b\)&lt;/span&gt;, and this method is consist of three st</summary>
      
    
    
    
    <category term="Mathematics" scheme="http://example.com/categories/Mathematics/"/>
    
    
    <category term="Linear Algebra" scheme="http://example.com/tags/Linear-Algebra/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Docker-Compose</title>
    <link href="http://example.com/2022/04/13/docker-compose/"/>
    <id>http://example.com/2022/04/13/docker-compose/</id>
    <published>2022-04-13T20:27:36.000Z</published>
    <updated>2022-04-13T21:58:12.233Z</updated>
    
    <content type="html"><![CDATA[<p>Docker-Compose is used to manage your containers, kind of like a container steward. We write a file, declare the container to be started in this file, configure some parameters, execute this file, Docker will start all containers according to the declared configuration.</p><p>Here is an example to build a multi-container Apache Spark cluster using docker-compose.</p><h2 id="create-a-file-name-dockerfile-and-add-the-following-contents-in-the-file">(1) Create a file name “Dockerfile ” and add the following contents in the file</h2><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">FROM</span> ubuntu:<span class="hljs-number">20</span>.<span class="hljs-number">04</span><br><span class="hljs-attribute">RUN</span> apt-get update<br><span class="hljs-attribute">RUN</span> apt-get -y upgrade<br><span class="hljs-attribute">RUN</span> apt install -y openjdk-<span class="hljs-number">8</span>-jre-headless<br><span class="hljs-attribute">RUN</span> apt install -y scala<br><span class="hljs-attribute">RUN</span> apt install -y wget<br><span class="hljs-attribute">RUN</span> apt install -y screen<br><span class="hljs-attribute">RUN</span> wget https://archive.apache.org/dist/spark/ spark-<span class="hljs-number">3</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span>/spark-<span class="hljs-number">3</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span>-bin-hadoop<span class="hljs-number">3</span>.<span class="hljs-number">2</span>.tgz<br><span class="hljs-attribute">RUN</span> tar xvf spark-<span class="hljs-number">3</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span>-bin-hadoop<span class="hljs-number">3</span>.<span class="hljs-number">2</span>.tgz<br><span class="hljs-attribute">RUN</span> mv spark-<span class="hljs-number">3</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span>-bin-hadoop<span class="hljs-number">3</span>.<span class="hljs-number">2</span>/ /usr/local/spark ENV PATH=<span class="hljs-string">"${PATH}:$SPARK_HOME/bin"</span><br><span class="hljs-attribute">ENV</span> SPARK_HOME=<span class="hljs-string">"/usr/local/spark"</span><br><span class="hljs-attribute">ENV</span> SPARK_NO_DAEMONIZE=<span class="hljs-string">"true"</span><br><span class="hljs-attribute">RUN</span> sleep <span class="hljs-number">15</span><br><span class="hljs-attribute">CMD</span> screen -d -m $SPARK_HOME/sbin/start-master.sh ; $SPARK_HOME/sbin/start-worker.sh spark://sparkmaster:<span class="hljs-number">7077</span><br></code></pre></td></tr></tbody></table></figure><h2 id="build-the-image-based-on-the-dockerfile-and-run-the-container">(2) Build the image based on the Dockerfile and run the container</h2><figure class="highlight applescript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs applescript">docker build -t sparkaio/<span class="hljs-keyword">first</span>:v0 .<br>docker <span class="hljs-built_in">run</span> -h sparkmaster &lt;Generated-Image-ID <span class="hljs-keyword">or</span> image <span class="hljs-built_in">name</span>&gt;<br></code></pre></td></tr></tbody></table></figure><p>The following commands are used to confirm that the spark setup is working correctly </p><figure class="highlight awk"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">docker exec -it container_id <span class="hljs-regexp">/bin/</span>bash<br><span class="hljs-variable">$SPARK_HOME</span><span class="hljs-regexp">/bin/</span>spark-submit --class org.apache.spark.examples.SparkPi --master spark:<span class="hljs-regexp">//</span>sparkmaster:<span class="hljs-number">7077</span> <span class="hljs-variable">$SPARK_HOME</span><span class="hljs-regexp">/examples/</span>jars/spark-examples_2.<span class="hljs-number">12</span>-<span class="hljs-number">3.2</span>.<span class="hljs-number">0</span>.jar<br></code></pre></td></tr></tbody></table></figure><p></p><p>Based on the above configurations, a single container-based Spark framework has been created.​ Next step is to prepare a configuration file, compatible with docker-compose, and run a Spark cluster with at least one master node and one additional worker.</p><h2 id="create-a-docker-compose.yml-file-and-write-the-configuration-on-this-file">(3) Create a docker-compose.yml file and write the configuration on this file</h2><p>Here, we set 1 master and 1 worker for the Spark cluster.</p><figure class="highlight dts"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">version:</span> <span class="hljs-string">"2"</span><br><span class="hljs-symbol">services:</span><br><span class="hljs-symbol">  master:</span><br><span class="hljs-symbol">    image:</span> sparkaio/first:v0<br><span class="hljs-symbol">    command:</span> <span class="hljs-meta-keyword">/usr/</span>local<span class="hljs-meta-keyword">/spark/</span>sbin/start-master.sh<br><span class="hljs-symbol">    hostname:</span> master<br><span class="hljs-symbol">    ports:</span><br>      - <span class="hljs-string">"6066:6066"</span><br>      - <span class="hljs-string">"7070:7070"</span><br>      - <span class="hljs-string">"8080:8080"</span><br>      - <span class="hljs-string">"50070:50070"</span><br><span class="hljs-symbol">  worker:</span><br><span class="hljs-symbol">    image:</span> sparkaio/first:v0<br><span class="hljs-symbol">    command:</span> <span class="hljs-meta-keyword">/usr/</span>local<span class="hljs-meta-keyword">/spark/</span>sbin/start-worker.sh<br><span class="hljs-symbol">    links:</span><br>      - master<br></code></pre></td></tr></tbody></table></figure><h2 id="startup-and-shutdown-of-docker-compose">(4) Startup and shutdown of docker-compose</h2><figure class="highlight crmsh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs crmsh"><span class="hljs-comment"># We can start the container by using up</span><br>docker-compose up<br><span class="hljs-comment"># After the first time, we can simply use start to start the services</span><br>docker-compose <span class="hljs-literal">start</span><br><span class="hljs-comment"># To safely stop the active services, we can use stop</span><br>docker-compose <span class="hljs-literal">stop</span><br><span class="hljs-comment"># To reset the status of the project, we simply run down</span><br>docker-compose down<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Docker-Compose is used to manage your containers, kind of like a container steward. We write a file, declare the container to be started </summary>
      
    
    
    
    <category term="Data Engineering" scheme="http://example.com/categories/Data-Engineering/"/>
    
    
    <category term="Docker" scheme="http://example.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Basic Use of Docker</title>
    <link href="http://example.com/2022/04/13/docker/"/>
    <id>http://example.com/2022/04/13/docker/</id>
    <published>2022-04-13T09:32:34.000Z</published>
    <updated>2022-04-13T20:31:56.714Z</updated>
    
    <content type="html"><![CDATA[<p>Docker is an open source application container that allows developers to package their applications and dependencies into a portable engine, and then publish them to operating system such as Linux or Windows for virtualization.</p><p>This blog covers basic introduction of the Docker environment, Dockerfile settings and construction of a multi-container environment using docker-compose (one of the essential tools to build, connect and run multiple containers).</p><h2 id="docker-containers">(1) Docker containers</h2><p>The following commands are used to install docker on virtual machine:</p><figure class="highlight smali"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs smali">sudo bash<br>apt update; apt -y upgrade;<br><span class="hljs-comment"># Install the required packages</span><br>apt install apt-transport-https ca-certificates curl software-properties-common<br><span class="hljs-comment"># Add the GPG key for the official Docker repository to the system</span><br>curl -fsSL https://download.docker.com/ linux/ubuntu/gpg |sudo apt-key<span class="hljs-built_in"> add </span>-<br><span class="hljs-comment"># Add the Docker repository to APT sources</span><span class="hljs-built_in"></span><br><span class="hljs-built_in">add-apt-repository </span><span class="hljs-string">"deb [arch=amd64] https:// download.docker.com/linux/ubuntu bionic stable"</span><br><span class="hljs-comment"># Update the package database with the Docker packages from the newly added repo</span><br>apt-get update<br><span class="hljs-comment"># Install Docker</span><br>apt install docker-ce<br><span class="hljs-comment"># Check if docker has started</span><span class="hljs-keyword"></span><br><span class="hljs-keyword">system</span>ctl status docker<br></code></pre></td></tr></tbody></table></figure><p>Then create a file name “Dockerfile ” and add the following contents in the file: </p><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-keyword">FROM</span> ubuntu:20.04<br><span class="hljs-builtin-name">RUN</span> apt-<span class="hljs-builtin-name">get</span> update<br><span class="hljs-builtin-name">RUN</span> apt-<span class="hljs-builtin-name">get</span> -y<span class="hljs-built_in"> upgrade</span><br><span class="hljs-built_in"></span><span class="hljs-builtin-name">RUN</span> apt-<span class="hljs-builtin-name">get</span> install sl<br>ENV <span class="hljs-attribute">PATH</span>=<span class="hljs-string">"<span class="hljs-variable">${PATH}</span>:/usr/games/"</span><br>CMD [<span class="hljs-string">"echo"</span>, <span class="hljs-string">"Data Engineering-I."</span>]<br></code></pre></td></tr></tbody></table></figure><p></p><p>Contextualize a container (create a new image that is contextualized according to the Dockerfile and start a container based on the contextualized image): </p><figure class="highlight applescript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs applescript">docker build -t mycontainer/<span class="hljs-keyword">first</span>:v1 .<br><span class="hljs-comment"># In batch mode</span><br>docker <span class="hljs-built_in">run</span> mycontainer/<span class="hljs-keyword">first</span>:v1<br><span class="hljs-comment"># In interactive mode:</span><br>docker <span class="hljs-built_in">run</span> -<span class="hljs-keyword">it</span> mycontainer/<span class="hljs-keyword">first</span>:v1 bash<br></code></pre></td></tr></tbody></table></figure><p></p><p>Now we have a running docker container with some extra packages installed in it.</p><h2 id="dockerhub">(2) DockerHub</h2><p><a href="https://hub.docker.com/">DockerHub</a> is used to store the Docker images, and developers also can use the images that are stored on DockerHub so the required development-environment can be built quickly. To use DockerHub, we need to create an account on its official website firstly. And the following commands are use to operate DockerHub on virtual machine.</p><figure class="highlight vala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs vala"><span class="hljs-meta"># login the DockerHub account on virtual machine</span><br>docker login<br><span class="hljs-meta"># then input the username and password</span><br><br><span class="hljs-meta"># upload image to DockerHub</span><br><span class="hljs-meta"># docker tag 'image-name:tag-name or image-ID' 'username/ image-name:tag-name'</span><br>docker tag e7083fd898c7 arnieswap/my_repo:testing<br>docker push arnieswap/my_repo<br><span class="hljs-meta"># check if it's pushed successfully</span><br>docker search arnieswap/my_repo<br><br><span class="hljs-meta"># download image from DockerHub</span><br>docker pull arnieswap/my_repo<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Docker is an open source application container that allows developers to package their applications and dependencies into a portable engi</summary>
      
    
    
    
    <category term="Data Engineering" scheme="http://example.com/categories/Data-Engineering/"/>
    
    
    <category term="Docker" scheme="http://example.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Norms and Condition Number</title>
    <link href="http://example.com/2022/03/28/subspace-1/"/>
    <id>http://example.com/2022/03/28/subspace-1/</id>
    <published>2022-03-28T12:27:12.000Z</published>
    <updated>2022-04-11T21:22:08.653Z</updated>
    
    <content type="html"><![CDATA[<p>Norm is a function from a real or complex vector space to the non-negative real numbers that behaves in certain ways, which can be used to measure the size of vectors (vector norm) and matrices (matrix norm). And norm is denoted by <span class="math inline">\(\parallel . \parallel\)</span>.</p><p>The condition number is an application of the derivative, and is formally defined as the value of the asymptotic worst-case relative change in output for a relative change in input. The "function" is the solution of a problem and the "arguments" are the data in the problem, so the condition number depends on the nature of the underlying problem.</p><h2 id="conditions-of-norms">Conditions of Norms</h2><p>To be valid and useful as norms, all norms must satisfy the following conditions:</p><ol type="1"><li><p><span class="math inline">\(\parallel A \parallel\)</span> <span class="math inline">\(\ge 0\)</span> for vectors <span class="math inline">\(\parallel x \parallel\)</span> <span class="math inline">\(\ge 0\)</span></p></li><li><p><span class="math inline">\(\parallel A \parallel\)</span> <span class="math inline">\(= 0\)</span> if and only if <span class="math inline">\(A=0\)</span></p></li><li><p><span class="math inline">\(\parallel \alpha A \parallel\)</span> = <span class="math inline">\(\mid \alpha\mid.\parallel A \parallel\)</span></p></li><li><p><span class="math inline">\(\parallel A+B \parallel\)</span> <span class="math inline">\(\le\)</span> <span class="math inline">\(\parallel A \parallel + \parallel B \parallel\)</span>, triangle inequality<br></p></li><li><p><span class="math inline">\(\parallel AB \parallel\)</span> <span class="math inline">\(\le\)</span> <span class="math inline">\(\parallel A \parallel . \parallel B \parallel\)</span>, Cauchy-Schwarz inequality also <span class="math inline">\(\parallel Ax \parallel\)</span> <span class="math inline">\(\le\)</span> <span class="math inline">\(\parallel A \parallel . \parallel x \parallel\)</span> is valid</p></li></ol><h2 id="vector-norms">Vector Norms</h2><p>2-norm is most used but which one to use depends on the application. Let <span class="math inline">\(x\)</span> be a vector of length <span class="math inline">\(n\)</span></p><ol type="1"><li><p>2-norm, Euclidian norm:<br><span class="math inline">\(\|x\|_{2}=\sqrt{\left|x_{1}\right|^{2}+\left|x_{2}\right|^{2}+\cdots+\left|x_{n}\right|^{2}}=\sqrt{x^{T} x}\)</span></p></li><li><p>1-norm, minimum norm:<br><span class="math inline">\(\|x\|_{1}=\left|x_{1}\right|+\left|x_{2}\right|+\cdots+\left|x_{n}\right|\)</span></p></li><li><p><span class="math inline">\(\infty\)</span> -norm, maximum norm:<br><span class="math inline">\(\|x\|_{\infty}=\max _{i}\left\{\left|x_{1}\right|, \ldots,\left|x_{n}\right|\right\}\)</span></p></li></ol><h2 id="matrix-norms">Matrix Norms</h2><ol type="1"><li><p>Frobenius norm (straightforward extension of the Euclidean norm):<br><span class="math inline">\(\|A\|_{F}=\sqrt{\sum_{i=1}^{m} \sum_{i=1}^{n} a_{i j}^{2}}\)</span></p></li><li><p>Another common matrix norm (<span class="math inline">\(Ax\)</span> and <span class="math inline">\(x\)</span> are vectors):<br><span class="math inline">\(\|A\|=\max _{x \neq 0} \frac{\|A x\|}{\|x\|}\)</span></p></li></ol><h2 id="norms">Norms</h2><ol type="1"><li><p>1-norm, min norm, <span class="math inline">\(l_1\)</span>-norm:<br><span class="math inline">\(\|A\|_{1} = \max _{1 \le j \le n} \sum_{i=1}^{m} \left|a_{ij}\right|\)</span></p></li><li><p><span class="math inline">\(\infty\)</span>-norm, max norm, <span class="math inline">\(l_{\infty}\)</span>-norm:<br><span class="math inline">\(\|A\|_{\infty} = \max _{1 \le i \le m} \sum_{j=1}^{n} \left|a_{ij}\right|\)</span></p></li><li><p>2-norm, Euclidian/spectral norm, <span class="math inline">\(l_2\)</span>-norm (<span class="math inline">\(\lambda(A^{T}A)\)</span> are the eigenvalues of <span class="math inline">\(A^{T}A\)</span>):<br><span class="math inline">\(\|A\|_{2} = \sqrt{\max \{ \lambda (A^{T}A)\}}\)</span><br></p></li></ol><h2 id="norm-calculation-in-python">Norm Calculation in Python</h2><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">A</span> = np.array([[<span class="hljs-number">1</span>,-<span class="hljs-number">2</span>],[<span class="hljs-number">6</span>,<span class="hljs-number">4</span>]])<br><span class="hljs-comment"># 1-norm</span><br><span class="hljs-attribute">np</span>.linalg.norm(A,<span class="hljs-number">1</span>)<br><span class="hljs-comment"># max-norm</span><br><span class="hljs-attribute">np</span>.linalg.norm(A,np.inf)<br><span class="hljs-comment"># 2-norm</span><br><span class="hljs-attribute">np</span>.linalg.norm(A,<span class="hljs-number">2</span>) <br><span class="hljs-comment"># Frobenius</span><br><span class="hljs-attribute">np</span>.linalg.norm(A,’fro’)<br><span class="hljs-comment"># Default</span><br><span class="hljs-attribute">np</span>.linalg.norm(A)            <br></code></pre></td></tr></tbody></table></figure><h2 id="characteristics-of-condition-number">Characteristics of Condition Number</h2><ol type="1"><li><p>A very big condition number suggests that matrix close to singular (<span class="math inline">\(cond \approx 10^{16}\)</span>), and it also means that the system is sensitive to these perturbations.</p></li><li><p>Best possible case: Condition number = 1<br><span class="math inline">\(cond(A)\)</span> <span class="math inline">\(=\)</span> <span class="math inline">\(\parallel A^{-1} \parallel . \parallel A \parallel\)</span> <span class="math inline">\(\le\)</span> <span class="math inline">\(\parallel A^{-1}A \parallel\)</span> <span class="math inline">\(=\)</span> <span class="math inline">\(\parallel I \parallel\)</span> <span class="math inline">\(= 1\)</span><br>meaning no magnification at all</p></li><li><p>No sharp definition for what’s large/small.</p></li><li><p>Condition number also tends to get bigger when matrix size increases.</p></li></ol><h2 id="condition-number-calculation-in-python">Condition Number Calculation in Python</h2><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">A</span> = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>]])<br><span class="hljs-comment"># 1-norm condition no.</span><br><span class="hljs-attribute">np</span>.linalg.cond(A,<span class="hljs-number">1</span>)      <br><span class="hljs-comment"># max-norm condition no.</span><br><span class="hljs-attribute">np</span>.linalg.cond(A,np.inf) <br><span class="hljs-comment"># 2-norm condition no.</span><br><span class="hljs-attribute">np</span>.linalg.cond(A,<span class="hljs-number">2</span>)      <br><span class="hljs-comment"># Frobenius condition no.</span><br><span class="hljs-attribute">np</span>.linalg.cond(A,’fro’)  <br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Norm is a function from a real or complex vector space to the non-negative real numbers that behaves in certain ways, which can be used t</summary>
      
    
    
    
    <category term="Mathematics" scheme="http://example.com/categories/Mathematics/"/>
    
    
    <category term="Linear Algebra" scheme="http://example.com/tags/Linear-Algebra/"/>
    
  </entry>
  
  <entry>
    <title>Subspace</title>
    <link href="http://example.com/2022/03/27/subspace/"/>
    <id>http://example.com/2022/03/27/subspace/</id>
    <published>2022-03-27T21:06:00.000Z</published>
    <updated>2022-04-07T11:10:20.883Z</updated>
    
    <content type="html"><![CDATA[<p>In Linear Algebra, linear subspace is also known as vector subspace, which is a subset of a larger <a href="https://imchengliang.top/2021/12/23/post/">vector space</a>. A linear subspace is normally called as subspace when the context serves to distinguish it from other types of subspaces.</p><p>For all subspace <span class="math inline">\(U\)</span>, they have to satisfy the following 3 rules:</p><ol type="1"><li><p>Additive identity <span class="math inline">\(0, u \in U\)</span>, and <span class="math inline">\(0+u = u+0\)</span>, <span class="math inline">\(0\)</span> and <span class="math inline">\(u\)</span> here are vectors.</p></li><li><p>Close under addition For all <span class="math inline">\(u, w \in U\)</span>, then <span class="math inline">\(u+w \in U\)</span>, <span class="math inline">\(u\)</span> and <span class="math inline">\(w\)</span> here are vectors.</p></li><li><p>Close under scalar multiplication For all <span class="math inline">\(u \in U\)</span>, <span class="math inline">\(k \in R\)</span>, then <span class="math inline">\(k*u \in U\)</span>, <span class="math inline">\(u\)</span> is a vector and <span class="math inline">\(k\)</span> is a constant.</p></li></ol><p>Suppose that <span class="math inline">\(A\)</span> is a <span class="math inline">\(m * n\)</span> matrix that maps vectors in <span class="math inline">\(R^n\)</span> to vectors in <span class="math inline">\(R^m\)</span>. The four fundamental subspaces associated with <span class="math inline">\(A\)</span>, two in <span class="math inline">\(R^n\)</span> and two in <span class="math inline">\(R^n\)</span>.</p><p><span class="math display">\[A=\left(\begin{array}{ll}a_{11} &amp; \dots &amp; a_{1n}\\\dots &amp; \dots &amp; \dots\\a_{m1} &amp; \dots &amp; a_{mn}\end{array}\right) = \left(\begin{array}{ll}1 &amp; 3 &amp; 3 \\2 &amp; 4 &amp; 6 \\1 &amp; 7 &amp; 3\end{array}\right) \in M_{m,n}(R) = M_{3,3}(R)\]</span></p><h2 id="column-space">Column Space</h2><p>The column space of A is the linear combination of all linearly independent non-zero columns in A (it works with or without Gaussian elimination), i.e. <span class="math inline">\(Ax\)</span>, subspace in <span class="math inline">\(R^m\)</span>.</p><p>So <span class="math inline">\(C(A)\)</span> span <span class="math inline">\(\left\{\left(\begin{array}{c}a_{11} \\ \vdots \\ a_{m 1}\end{array}\right) \cdots\left(\begin{array}{c}a_{1 n} \\ \vdots \\ a_{m n}\end{array}\right) \right\}\)</span> , and the rank of A is equal to the dimension of column space.</p><p>We apply Gaussian elimination on matrix A, then get <span class="math inline">\(A=\left(\begin{array}{lll} 1 &amp; 3 &amp; 3 \\ 2 &amp; 4 &amp; 6 \\ 1 &amp; 7 &amp; 3 \end{array}\right) {\longrightarrow}\left(\begin{array}{lll} 1 &amp; 0 &amp; 3 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}\right)\)</span>.</p><p>Because column 1 and column 3 are not linearly independent, <span class="math inline">\(C(A)\)</span> is the linear combination of column 2 and column 1 or column 2 and column 3, either of these two is correct.</p><p>If we choose column 1 and column 2, then <span class="math inline">\(C(A) = \left\{\left(\begin{array}{c}1 \\ 0 \\ 0\end{array}\right) , \left(\begin{array}{c}0 \\ 1 \\ 0\end{array}\right) \right\}\)</span>, and <span class="math inline">\(Rank(A) = dim(C(A)) = 2\)</span>.</p><h2 id="row-space">Row Space</h2><p>The row space of A is the linear combination of all non-zero rows in A after Gaussian elimination, i.e. <span class="math inline">\(A^{T}y\)</span>, subspace in <span class="math inline">\(R^n\)</span>.</p><p>So <span class="math inline">\(R(A) = C(A^T)\)</span> span <span class="math inline">\(\left\{\left(a_{11} \ldots a_{1 n}\right) \cdots\left(a_{m 1} \cdots a_{m n}\right)\right\}\)</span>.</p><p>According to Gaussian elimination, we get <span class="math inline">\(A {\rightarrow}\left(\begin{array}{lll} 1 &amp; 0 &amp; 3 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}\right)\)</span>, then row space of A is the non-zero rows (row 1 and row 2), <span class="math inline">\(R(A)=\left\{\left(1, 0, 3\right),\left(0, 1, 0\right)\right\}\)</span>.</p><h2 id="nullspace">Nullspace</h2><p>The nullspace of A is the linear combination of all solution of <span class="math inline">\(Ax = 0\)</span>, subspace in <span class="math inline">\(R^n\)</span>.</p><p>According to Gaussian elimination, we get <span class="math inline">\(A {\rightarrow}\left(\begin{array}{lll} 1 &amp; 0 &amp; 3 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}\right)\)</span>, then we get<br></p><p><span class="math inline">\(\left(\begin{array}{lll}1 &amp; 0 &amp; 3 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0\end{array}\right) \cdot\left(\begin{array}{l}x_{1} \\ x_{2} \\ x_{3}\end{array}\right)=\left\{\begin{array}{c}x_{1}+3 x_{3}=0 \\ x_{2}=0\end{array} \rightarrow\left\{\begin{array}{c}x_{1}=-3 x_{3} \\ x_{2}=0 \\ x_{3} \in R\end{array} \rightarrow x=N(A)=\left(\begin{array}{c}-3 \\ 0 \\ 1\end{array}\right) \cdot x_{3}\right.\right.\)</span><br></p><p>So a basis of <span class="math inline">\(N(A)\)</span> is <span class="math inline">\((-3, 0, 1 )\)</span>, and nullity of A: <span class="math inline">\(null(A) = dim(N(A)) = 1\)</span>.</p><h2 id="left-nullspace">Left Nullspace</h2><p>The left nullspace of A is the linear combination of all solution of <span class="math inline">\(A^{T}y = 0\)</span>, subspace in <span class="math inline">\(R^m\)</span>.</p><p>We apply Gaussian elimination on matrix <span class="math inline">\(A^T\)</span>, then get <span class="math inline">\(A^T=\left(\begin{array}{lll} 1 &amp; 2 &amp; 1 \\ 3 &amp; 4 &amp; 7 \\ 3 &amp; 6 &amp; 3 \end{array}\right) {\longrightarrow}\left(\begin{array}{lll} 1 &amp; 0 &amp; 5 \\ 0 &amp; 1 &amp; -2 \\ 0 &amp; 0 &amp; 0 \end{array}\right)\)</span><br></p><p><span class="math inline">\(\left(\begin{array}{lll}1 &amp; 0 &amp; 5 \\ 0 &amp; 1 &amp; -2 \\ 0 &amp; 0 &amp; 0\end{array}\right) \cdot\left(\begin{array}{l}y_{1} \\ y_{2} \\ y_{3}\end{array}\right)=\left\{\begin{array}{c}y_{1}+5 y_{3}=0 \\ y_{2}-2y_{3}=0\end{array} \rightarrow\left\{\begin{array}{c}y_{1}=-5 y_{3} \\ y_{2}=2 y_{3} \\ y_{3} \in R\end{array} \rightarrow y=N(A^T)=\left(\begin{array}{c}-5 \\ 2 \\ 1\end{array}\right) \cdot y_{3}\right.\right.\)</span> &nbsp;<br></p><p>So a basis of <span class="math inline">\(N(A^T)\)</span> is <span class="math inline">\((-5, 2, 1 )\)</span>, and <span class="math inline">\(null(A^T) = dim(N(A^T)) = 1\)</span>.</p><h2 id="characteristic-of-these-four-subspaces">Characteristic of these four subspaces</h2><ol type="1"><li>Orthogonal: Column space and left nullspace are orthogonal, row space and null space are orthogonal.<br></li></ol><p><span class="math inline">\(C(A) \perp N\left(A^{\top}\right) \rightarrow(1,2,1) \cdot\left(\begin{array}{c}-5 \\ 2 \\ 1\end{array}\right)=(3,4,7) \cdot\left(\begin{array}{c}-5 \\ 2 \\ 1\end{array}\right)=0\)</span><br></p><p><span class="math inline">\(R(A) \perp N(A) \rightarrow(1,0,3) \cdot\left(\begin{array}{c}-3 \\ 0 \\ 1\end{array}\right)=(0,1,0) \cdot\left(\begin{array}{c}-3 \\ 0 \\ 1\end{array}\right)=0\)</span></p><ol start="2" type="1"><li>Rank-nullity:</li></ol><p>Rank + Nullity = Column Number<br></p><p>dim(Row Space) + dim(Left Nullsapce) = Row Number</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;In Linear Algebra, linear subspace is also known as vector subspace, which is a subset of a larger &lt;a href=&quot;https://imchengliang.top/2021</summary>
      
    
    
    
    <category term="Mathematics" scheme="http://example.com/categories/Mathematics/"/>
    
    
    <category term="Linear Algebra" scheme="http://example.com/tags/Linear-Algebra/"/>
    
  </entry>
  
  <entry>
    <title>Run Word-Count Example of Hadoop (Python Version)</title>
    <link href="http://example.com/2022/02/16/hadoop-on-cloud-python/"/>
    <id>http://example.com/2022/02/16/hadoop-on-cloud-python/</id>
    <published>2022-02-16T13:59:18.000Z</published>
    <updated>2022-04-13T09:21:22.689Z</updated>
    
    <content type="html"><![CDATA[<p>While Hadoop/MapReduce is based on Java, it is not necessary to use Java to write the mapper and reducer. The Hadoop framework provides the “Streaming API”, which lets us use any command line executable that reads from standard input and writes to standard output as the mapper or reducer. This tutorial (<a href="https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">Link</a>), although a bit old, provides an excellent introductory example to using Python and Hadoop streaming.</p><h2 id="case-description">(1) Case Description</h2><p>In this blog, I'm going to use a word-count example to show how to run MadReduce task by Python. The data I used in this example is twitter data (json), and the goal is to count the occurrence of a list of words. To achieve the goal, I would revise the Python code in the tutorial above.</p><h2 id="adjust-the-code-for-python-hadoop-example">(2) Adjust the Code For Python-Hadoop-Example</h2><p>The tutorial above uses mapper.py and reducer.py to run MapReduce task but these two files are in Python 2.7, which is out of date. Therefore, I would change it into Python 3.+ format and adjust it according to the case requirement.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">"""mapper.py"""</span><br><br><span class="hljs-keyword">import</span> sys <br><span class="hljs-keyword">import</span> json <br><span class="hljs-keyword">import</span> re<br>pronouns = [<span class="hljs-string">"han"</span>, <span class="hljs-string">"hon"</span>, <span class="hljs-string">"den"</span>, <span class="hljs-string">"det"</span>, <span class="hljs-string">"denna"</span>, <span class="hljs-string">"denne"</span>, <span class="hljs-string">"hen"</span>, <span class="hljs-string">"unique_tweet"</span>]<br><span class="hljs-comment"># input comes from STDIN (standard input) </span><br><span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> sys.stdin:<br>    <span class="hljs-comment"># remove leading and trailing whitespace </span><br>    line = line.strip()<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(line) != <span class="hljs-number">0</span>:<br>    <span class="hljs-comment"># use json to load the tweets</span><br>        jsonData = json.loads(line)<br>        <span class="hljs-comment"># determine whether it's retweet or not if 'retweeted_status' not in jsonData:</span><br>        tweets = jsonData[<span class="hljs-string">'text'</span>]<br>        <span class="hljs-comment"># split the text into words</span><br>        pattern = re.<span class="hljs-built_in">compile</span>(<span class="hljs-string">r"\w+"</span>)<br>        words = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">str</span>, pattern.findall(tweets))) <br>        <span class="hljs-comment"># increase counters </span><br>        words.append(<span class="hljs-string">'unique_tweet'</span>)<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>            word = word.lower() <br>            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> pronouns:<br>            <span class="hljs-comment"># write the results to STDOUT (standard output);</span><br>            <span class="hljs-comment"># what we output here will be the input for the</span><br>            <span class="hljs-comment"># Reduce step, i.e. the input for reducer.py #</span><br>            <span class="hljs-comment"># tab-delimited; the trivial word count is 1</span><br>                <span class="hljs-built_in">print</span>(word, <span class="hljs-number">1</span>)<br></code></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">"""reducer.py"""</span><br><br><span class="hljs-keyword">import</span> sys <br><span class="hljs-keyword">from</span> operator <span class="hljs-keyword">import</span> itemgetter<br><br>current_word = <span class="hljs-literal">None</span><br>current_count = <span class="hljs-number">0</span> <br>word = <span class="hljs-literal">None</span><br><span class="hljs-comment"># input comes from STDIN  </span><br><span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> sys.stdin:<br>    <span class="hljs-comment"># remove leading and trailing whitespace </span><br>    line = line.strip()<br>    <span class="hljs-comment"># parse the input we got from mapper.py word, </span><br>    count = line.split(<span class="hljs-string">'\t'</span>, <span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># convert count (currently a string) to int </span><br>    <span class="hljs-keyword">try</span>:<br>        count = <span class="hljs-built_in">int</span>(count) <br>    <span class="hljs-keyword">except</span> ValueError:<br>        <span class="hljs-comment"># count was not a number, so silently </span><br>        <span class="hljs-comment"># ignore/discard this line</span><br>        <span class="hljs-keyword">continue</span><br>    <span class="hljs-comment"># this IF-switch only works because Hadoop sorts map output </span><br>    <span class="hljs-comment"># by key (here: word) before it is passed to the reducer</span><br>    <span class="hljs-keyword">if</span> current_word == word:<br>        current_count += count <br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">if</span> current_word:<br>            <span class="hljs-comment"># write result to STDOUT </span><br>            <span class="hljs-built_in">print</span>(current_word, current_count)<br>        current_count = count <br>        current_word = word<br>    <span class="hljs-comment"># don't forget to output the last word if needed! </span><br>    <span class="hljs-keyword">if</span> current_word == word:<br>        <span class="hljs-built_in">print</span>(current_word, current_count)<br></code></pre></td></tr></tbody></table></figure><h2 id="run-the-code-on-hadoop">(3) Run the Code on Hadoop</h2><p>Before we actually run the task on Hadoop, it's better to have a local test about mapper.py and reducer.py on small data set to check if it's able to run successfully. The command for local test is given as below.</p><figure class="highlight gradle"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gradle"># The format is <span class="hljs-string">'cat /path-of-data-set | /path-of-mapper.py  | sort -k1,1 | /path-of-reducer.py'</span><br>cat <span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/tweets/</span>tweets_0.txt | <span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/python/m</span>apper.py | <span class="hljs-keyword">sort</span> -k1,<span class="hljs-number">1</span> | <span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/python/</span>reducer.py<br></code></pre></td></tr></tbody></table></figure><p>If we can get the expected result on the test, then we can run the Hadoop task. First, we put the data set into HDFS, and then run these two Python file on Hadoop.</p><figure class="highlight awk"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># The format is 'hadoop fs -put /path-of-data-set /folder-in-HDFS'</span><br>hadoop fs -put <span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/test1/</span>RC_2005-<span class="hljs-number">12</span> /test1<br><br><span class="hljs-comment"># Check if the data is in HDFS</span><br>hadoop fs -ls /test1<br><br><span class="hljs-comment"># Run the Python-Hadoop task</span><br>bin<span class="hljs-regexp">/hadoop jar contrib/</span>streaming/hadoop-*streaming*.jar \<br>-file <span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/python/m</span>apper.py    -mapper <span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/python/m</span>apper.py \<br>-file <span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/python/</span>reducer.py   -reducer <span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/python/</span>reducer.py \<br>-input <span class="hljs-regexp">/test1/</span>*  -output /test1-output<br><br><span class="hljs-comment"># Check the output file</span><br>hadoop fs -cat <span class="hljs-regexp">/test1-output/</span>part-<span class="hljs-number">00000</span><br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;While Hadoop/MapReduce is based on Java, it is not necessary to use Java to write the mapper and reducer. The Hadoop framework provides t</summary>
      
    
    
    
    <category term="Data Engineering" scheme="http://example.com/categories/Data-Engineering/"/>
    
    
    <category term="Hadoop" scheme="http://example.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Run Word-Count Example of Hadoop (Java Version)</title>
    <link href="http://example.com/2022/02/15/hadoop-on-cloud/"/>
    <id>http://example.com/2022/02/15/hadoop-on-cloud/</id>
    <published>2022-02-15T12:19:11.000Z</published>
    <updated>2022-04-13T09:21:29.557Z</updated>
    
    <content type="html"><![CDATA[<p>The main content of this blog is introducing how to run word count example by Hadoop and Java on the cloud, including set up the Hadoop configuration, generate and adjust the Java code for Hadoop, and commands of running Hadoop. Before this, make sure that <a href="https://imchengliang.top/2022/01/21/cloud/">Linux has been installed on your cloud service</a>, both Java and Hadoop are installed on your Linux system.</p><h2 id="word-count-example-in-local-standalone-mode">(1) Word Count Example in Local (Standalone) Mode</h2><p>The Standalone mode refers to an independent Java process that runs on a host and runs in non-distributed mode by default. It uses a local file system instead of a distributed file system and does not require any Hadoop daemons to be loaded.</p><p>The default mode for Hadoop is standalone mode, so once hadoop is installed, we just need to put to data into input folder and run the command directly.</p><p>Firstly, we create the input folder on local filesystem, and then we download the data that can be used for word-count from internet and put it into the input folder. </p><figure class="highlight awk"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># On Ubuntu linux ‘wget’ command can be used to download data over http</span><br>wget http:<span class="hljs-regexp">//</span>www.gutenberg.org<span class="hljs-regexp">/ebooks/</span><span class="hljs-number">20417</span>.txt.utf-<span class="hljs-number">8</span><br></code></pre></td></tr></tbody></table></figure><p></p><p>After the data is ready, we can run the following command (there is only one command!) to run the canonical MapReduce example.</p><figure class="highlight awk"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/hadoop-3.3.1/</span>bin<span class="hljs-regexp">/hadoop jar /u</span>sr<span class="hljs-regexp">/local/</span>hadoop-<span class="hljs-number">3.3</span>.<span class="hljs-number">1</span><span class="hljs-regexp">/share/</span>hadoop<span class="hljs-regexp">/mapreduce/</span>hadoop*examples*.jar wordcount <span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/wordcount/i</span>nput <span class="hljs-regexp">/home/u</span>buntu<span class="hljs-regexp">/wordcount/</span>output<br></code></pre></td></tr></tbody></table></figure><ul><li>The format of this command: /installation directory of Hadoop/bin/hadoop jar /installation directory of Hadoop/share/hadoop/mapreduce/hadoop<em>examples</em>.jar wordcount /path of stored-data /path for the output</li></ul><p>Note: the folder names “input” and “output” are arbitrary - you can choose any names you like for the input and output directories. But the “input” folder should contain the downloaded file. Note that the output folder is created by Hadoop, so you should not create it manually before running the command above.</p><h2 id="word-count-example-in-pseudo-distributed-mode">(2) Word Count Example in Pseudo-Distributed Mode</h2><p>The Pseudo-distributed mode refers to running on one host, using multiple Java processes to imitate various nodes that run in fully distributed mode but not actually be used in production. The pseudo-distributed mode has the main functions of the fully distributed mode.</p><p>Firstly, we need to edit the configuration file to switch Hadoop into pseudo-distributed mode. The two configuration files that are needed to be edited are 'core-site.xml' and 'hdfs-site.xml', and they are located on '/installation directory of Hadoop/etc/hadoop'.</p><figure class="highlight dts"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-meta"># core-site.xml</span><br><span class="hljs-params">&lt;configuration&gt;</span><br>    <span class="hljs-params">&lt;property&gt;</span><br>        <span class="hljs-params">&lt;name&gt;</span>fs.defaultFS<span class="hljs-params">&lt;/name&gt;</span><br>        <span class="hljs-params">&lt;value&gt;</span>hdfs:<span class="hljs-comment">//localhost:9000&lt;/value&gt;</span><br>    <span class="hljs-params">&lt;/property&gt;</span><br><span class="hljs-params">&lt;/configuration&gt;</span><br></code></pre></td></tr></tbody></table></figure><figure class="highlight dts"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-meta"># hdfs-site.xml</span><br><span class="hljs-params">&lt;configuration&gt;</span><br>    <span class="hljs-params">&lt;property&gt;</span><br>        <span class="hljs-params">&lt;name&gt;</span>dfs.replication<span class="hljs-params">&lt;/name&gt;</span><br>        <span class="hljs-params">&lt;value&gt;</span><span class="hljs-number">1</span><span class="hljs-params">&lt;/value&gt;</span><br>    <span class="hljs-params">&lt;/property&gt;</span><br><span class="hljs-params">&lt;/configuration&gt;</span><br></code></pre></td></tr></tbody></table></figure><ul><li>We configurate the global parameters of cluster including HDFS URL and temporary directory of Hadoop by editing core-site.xml. The parameters of HDFS are stored in hdfs-site.xml, such as the storage location of the name node and data node, the number of file copies, the read permission of the file, etc.</li></ul><p>After the configuration files are ready, let's check that if we can ssh to the localhost without a passphrase.</p><figure class="highlight elixir"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$ </span>ssh localhost<br></code></pre></td></tr></tbody></table></figure><p>If we cannot ssh to localhost without a passphrase, execute the following commands to setup passphraseless ssh</p><figure class="highlight arcade"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs arcade">$ ssh-keygen -t rsa -P <span class="hljs-string">''</span> -f ~<span class="hljs-regexp">/.ssh/i</span>d_rsa<br>$ cat ~<span class="hljs-regexp">/.ssh/i</span>d_rsa.pub &gt;&gt; ~<span class="hljs-regexp">/.ssh/</span>authorized_keys<br>$ chmod <span class="hljs-number">0600</span> ~<span class="hljs-regexp">/.ssh/</span>authorized_keys<br></code></pre></td></tr></tbody></table></figure><p>The next step is to use the following commands in the installation directory of Hadoop to start a MapReduce job in pseudo-distributed mode locally. </p><figure class="highlight awk"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># Format the filesystem</span><br>$ bin/hdfs namenode -format<br><span class="hljs-comment"># Start NameNode daemon and DataNode daemon</span><br>$ sbin/start-dfs.sh<br><span class="hljs-comment"># Make the HDFS directories required to execute MapReduce jobs</span><br>$ bin<span class="hljs-regexp">/hdfs dfs -mkdir /u</span>ser<br>$ bin<span class="hljs-regexp">/hdfs dfs -mkdir /u</span>ser/&lt;username&gt;<br></code></pre></td></tr></tbody></table></figure><p></p><p>We can check if the job starts successfully by the command 'jps'. </p><figure class="highlight vala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs vala">$ jps<br><span class="hljs-meta"># if the following namenode, datanode, secondarynamenode are all shown, it means that the job has started</span><br><span class="hljs-meta"># 6818 DataNode</span><br><span class="hljs-meta"># 6651 NameNode</span><br><span class="hljs-meta"># 7067 SecondaryNameNode</span><br><span class="hljs-meta"># 91869 Jps</span><br></code></pre></td></tr></tbody></table></figure><p></p><ul><li>DataNode stores data block, NameNode is to accept the read and write requests from client and send them to DataNode, SecondaryNameNode helps NameNode merge edits log to reduce NameNode’s startup time.</li></ul><h2 id="create-and-modify-the-java-code-for-word-count-example">(3) Create and Modify the Java code for Word Count Example</h2><p>Now we write the Java code to count the words in the txt file that is downloaded in step (1), the goal is that the output of the job should thus be a file containing lines like: a number_of_words_starting_with_a_or_A</p><p>First of all, we create and edit the Java file in any path of the local filesystem we want. </p><figure class="highlight irpf90"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs irpf90"><span class="hljs-keyword">touch</span> &lt;Java <span class="hljs-keyword">file</span> <span class="hljs-keyword">name</span>&gt;.java<br>vim &lt;Java <span class="hljs-keyword">file</span> <span class="hljs-keyword">name</span>&gt;,java<br></code></pre></td></tr></tbody></table></figure><p></p><p>The Java code is copied from the <a href="https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">official sample of Apache Hadoop</a>, and then adjust the code based on the requirement above. The two key function mapper and reducer after adjustment are shown as below, the full code can be find in <a href="https://github.com/Imchengliang/Data-Engineering">there</a> </p><figure class="highlight angelscript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs angelscript"><span class="hljs-keyword">public</span> static <span class="hljs-keyword">class</span> <span class="hljs-symbol">FirstLetterCountMapper</span><br><span class="hljs-symbol">extends</span> <span class="hljs-symbol">Mapper</span>&lt;<span class="hljs-symbol">LongWritable, <span class="hljs-symbol">Text</span>, <span class="hljs-symbol">Text</span>, <span class="hljs-symbol">IntWritable</span></span>&gt; {<br>    @Override<br><span class="hljs-keyword">public</span> <span class="hljs-built_in">void</span> map(LongWritable key, Text value, Context context) <br>throws IOException, InterruptedException {<br># change the input data <span class="hljs-built_in">int</span>o <span class="hljs-built_in">string</span> <span class="hljs-keyword">and</span> make all the letters of it be lowercase<br>String line = value.toString().toLowerCase();<br># split the <span class="hljs-built_in">string</span> data <span class="hljs-built_in">int</span>o individual word by blankspace<br>String[] <span class="hljs-built_in">string</span>s = line. split(<span class="hljs-string">" "</span>);<br><span class="hljs-keyword">for</span>(<span class="hljs-built_in">int</span> i=<span class="hljs-number">0</span>; i&lt;<span class="hljs-built_in">string</span>s.length; i++) {<br># filter <span class="hljs-keyword">out</span> empty <span class="hljs-built_in">string</span>s<br><span class="hljs-keyword">if</span> ( <span class="hljs-built_in">string</span>s[i] != <span class="hljs-literal">null</span> &amp;&amp; !<span class="hljs-built_in">string</span>s[i].isEmpty() ){<br># extract the first letter of <span class="hljs-keyword">this</span> word<br>char firstLetter = <span class="hljs-built_in">string</span>s[i].charAt(<span class="hljs-number">0</span>);<br># find the words begin with a-z <span class="hljs-keyword">and</span> deliver their first letter to reducer <span class="hljs-keyword">class</span><br>    <span class="hljs-symbol">if</span> (<span class="hljs-symbol">firstLetter</span> &gt;= '<span class="hljs-symbol">a</span>' &amp;&amp; <span class="hljs-symbol">firstLetter</span> &lt;= '<span class="hljs-symbol">z</span>') {<br>        context.write(new Text(String.valueOf(firstLetter)), new IntWritable(<span class="hljs-number">1</span>));<br>}<br>                }<br>            }<br>}<br>    } <br><br><span class="hljs-keyword">public</span> static <span class="hljs-keyword">class</span> <span class="hljs-symbol">FirstLetterCountReducer</span><br><span class="hljs-symbol">extends</span> <span class="hljs-symbol">Reducer</span>&lt;<span class="hljs-symbol">Text, <span class="hljs-symbol">IntWritable</span>, <span class="hljs-symbol">Text</span>, <span class="hljs-symbol">IntWritable</span></span>&gt; {<br><span class="hljs-keyword">private</span> IntWritable result = new IntWritable();<br>@Override<br><span class="hljs-keyword">public</span> <span class="hljs-built_in">void</span> reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)  <br>throws IOException, InterruptedException {<br><span class="hljs-built_in">int</span> sum = <span class="hljs-number">0</span>;<br># count the number of words begin with a-z <span class="hljs-keyword">in</span> a loop<br><span class="hljs-keyword">for</span> (IntWritable val : values) {<br>sum += val.<span class="hljs-keyword">get</span>();<br>            }<br>result.<span class="hljs-keyword">set</span>(sum);<br>context.write(key, result);<br>       }<br>    }<br></code></pre></td></tr></tbody></table></figure><p></p><p>After the file edition is finished, we need to compile the wordcount example and make a jar file. </p><figure class="highlight applescript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs applescript">cd /directory <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> folder <span class="hljs-keyword">that</span> <span class="hljs-keyword">contains</span> Java <span class="hljs-built_in">file</span><br>javac -cp `/installation directory <span class="hljs-keyword">of</span> Hadoop/bin/hadoop classpath` &lt;Java <span class="hljs-built_in">file</span> <span class="hljs-built_in">name</span>&gt;.java<br>jar -cvf &lt;Java <span class="hljs-built_in">file</span> <span class="hljs-built_in">name</span>&gt;.jar  *.<span class="hljs-built_in">class</span><br></code></pre></td></tr></tbody></table></figure><p></p><p>Now there is a file “<java file="" name="">.jar” in the same folder contains Java file. Next, load the file whose words we are counting into hdfs. </java></p><figure class="highlight livecodeserver"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">/installation <span class="hljs-built_in">directory</span> <span class="hljs-keyword">of</span> Hadoop/bin/hdfs dfs -<span class="hljs-built_in">put</span> /<span class="hljs-built_in">directory</span> <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> data we are going <span class="hljs-built_in">to</span> <span class="hljs-built_in">process</span><br></code></pre></td></tr></tbody></table></figure><p></p><ul><li>There should be a file in hdfs now, and we can check it by this command: /installation directory of Hadoop/bin/hdfs dfs -ls input</li></ul><p>Then, we run the following command to run the Java code in Hadoop. </p><figure class="highlight applescript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs applescript">/installation directory <span class="hljs-keyword">of</span> Hadoop/bin/hadoop jar &lt;Java <span class="hljs-built_in">file</span> <span class="hljs-built_in">name</span>&gt;.jar &lt;Java <span class="hljs-built_in">file</span> <span class="hljs-built_in">name</span>&gt; input output<br></code></pre></td></tr></tbody></table></figure><p></p><p>If the hadoop run completes normally, verify that the output looks as expected. First check the content of the output directory in hdfs. Then check the content of the output file using the ‘-cat’ argument to ‘hdfs dfs’. </p><figure class="highlight awk"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/installation directory of Hadoop/</span>bin/hdfs dfs -ls output<br><span class="hljs-regexp">/installation directory of Hadoop/</span>bin<span class="hljs-regexp">/hdfs dfs -cat output/</span>part-r-<span class="hljs-number">00000</span><br></code></pre></td></tr></tbody></table></figure><p></p><ul><li>The output files are by default named part-x-yyyyy where: x is either 'm' or 'r', depending on whether the job was a map only job, or reduce. yyyyy is the mapper or reducer task number (zero based).</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;The main content of this blog is introducing how to run word count example by Hadoop and Java on the cloud, including set up the Hadoop c</summary>
      
    
    
    
    <category term="Data Engineering" scheme="http://example.com/categories/Data-Engineering/"/>
    
    
    <category term="Hadoop" scheme="http://example.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Show the Hidden File on Mac OS</title>
    <link href="http://example.com/2022/01/29/hidden-file/"/>
    <id>http://example.com/2022/01/29/hidden-file/</id>
    <published>2022-01-29T10:09:26.000Z</published>
    <updated>2022-04-07T11:36:03.283Z</updated>
    
    <content type="html"><![CDATA[<p>Sometimes we might need to edit the hidden file on Mac such as the configuration file but we don't know the path of this file. Therefore, letting the computer shows the hidden file allows us to locate and edit this file more convenient. This post would introduce two ways to show the hidden file on Mac.</p><h2 id="enter-the-command-of-show-hide-hidden-file">(1) Enter the command of show / hide hidden file</h2><p>We can directly enter the following commands on terminal to show or hide all the hidden files. </p><figure class="highlight applescript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs applescript"><span class="hljs-comment"># Show the hidden files</span><br>defaults <span class="hljs-built_in">write</span> com.apple.finder AppleShowAllFiles -<span class="hljs-built_in">boolean</span> <span class="hljs-literal">true</span>;killall Finder<br><span class="hljs-comment"># Hide the hidden files</span><br>defaults <span class="hljs-built_in">write</span> com.apple.finder AppleShowAllFiles -<span class="hljs-built_in">boolean</span> <span class="hljs-literal">false</span>;killall Finder<br></code></pre></td></tr></tbody></table></figure><p></p><h2 id="create-the-sh-file-that-contains-the-two-commands-above-and-use-the-file-to-show-hide-the-hidden-files">(2) Create the SH file that contains the two commands above, and use the file to show / hide the hidden files</h2><p>The commands above is so long that it's hard to remember and type it correctly. Hence, we can store them in the SH file, and each time we run the SH file can also achieve the same effect. </p><figure class="highlight jboss-cli"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-comment"># Create the SH files </span><br>touch show_hidden_file.sh<br>touch hide_hidden_file.sh<br><br><span class="hljs-comment"># Write the show_hidden_files_command into show_hidden_file.sh</span><br><span class="hljs-comment"># Write the hide_hidden_files_command into hide_hidden_file.sh</span><br><br><span class="hljs-comment"># Show the hidden files</span><br><span class="hljs-string">./show_hidden_file.sh</span><br><span class="hljs-comment"># Hide the hidden files</span><br><span class="hljs-string">./hide_hidden_file.sh</span><br></code></pre></td></tr></tbody></table></figure><p></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Sometimes we might need to edit the hidden file on Mac such as the configuration file but we don&#39;t know the path of this file. Therefore,</summary>
      
    
    
    
    <category term="Data Engineering" scheme="http://example.com/categories/Data-Engineering/"/>
    
    
    <category term="Mac" scheme="http://example.com/tags/Mac/"/>
    
  </entry>
  
  <entry>
    <title>Use SSH to connect the virtual machine on cloud and run jupyter notebook on it</title>
    <link href="http://example.com/2022/01/21/cloud/"/>
    <id>http://example.com/2022/01/21/cloud/</id>
    <published>2022-01-21T08:36:42.000Z</published>
    <updated>2022-08-13T08:37:14.833Z</updated>
    
    <content type="html"><![CDATA[<p>In this blog, I will introduce how to create a virtual machine on openstack (it's a cloud platform), and then using SSH to connect the virtual machine with local computer. After the connection, I will show how to run jupyter notebook on this remote virtual machine.</p><h2 id="create-a-ssh-keypair-on-openstack">(1) Create a SSH-keypair on openstack</h2><p>The only method allowed to access the cloud instances is via ssh-keypairs. Username/Password is disabled by default on all cloud instances (according to best practice) and should never be enabled for security reasons.</p><p>The OpenStack software helps us create/import keys, and will make sure that the public keys are injected in the instances you create. The private key should be private and is for us to safekeep on our clients.</p><p>In the OpenStack dashboard: Compute -&gt; Key Pairs -&gt; Create Key Pair</p><p>Save the downloaded .pem file in a secure location on computer. We would need it when we want to access the virtual machine instance but we don't need to create a new ssh-keypair each time. <img src="/img/ssh_vm/ssh.png"></p><h2 id="launch-an-instance-of-virtual-machine-on-openstack">(2) Launch an instance of virtual machine on openstack</h2><p>In the OpenStack dashboard: Compute -&gt; Instances -&gt; Launch instance</p><p>Then we can choose the configuration we want for the virtual machine. For my instance, I choose ubuntu 20.04 as the image, 1 VCPUS, 512 MB RAM and 20 GB disk. We also have to give this instance a name, and it would auto select to use the key-pair we create. As for the rest of the configuration, I chose the default for them. <img src="/img/ssh_vm/configuration.png"></p><h2 id="use-ssh-to-access-the-virtual-machine-from-local-computer">(3) Use SSH to access the virtual machine from local computer</h2><p>Before the SSH connection, we need to associate a floating IP to the instance, which represents the IP address of this virtual machine. After the IP setting, we should change the permission of the downloaded-keypair file and SSH file so that the IP can access the key and connect with local computer. Run the following commands in terminal to change permission. "/..." is the path of this file on your computer. </p><figure class="highlight awk"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">chmod <span class="hljs-number">400</span> <span class="hljs-regexp">/.../</span>liang_cheng.pem<br>chmod <span class="hljs-number">700</span> <span class="hljs-regexp">/.../</span>.ssh<br>chmod <span class="hljs-number">600</span> <span class="hljs-regexp">/.../</span>.ssh/authorized_keys<br></code></pre></td></tr></tbody></table></figure><p></p><p>After the permission-change, we are able to use SSH to access the virtual machine by running the commands below.</p><figure class="highlight awk"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># if the instance uses old volume or snapshot but not a new image as the boot source, </span><br><span class="hljs-comment"># we need to run the next command first before run ssh -i </span><br>ssh-keygen -R floating IP<br>ssh -i <span class="hljs-regexp">/.../</span>liang_cheng.pem ubuntu@floating IP<br></code></pre></td></tr></tbody></table></figure><p><img src="/img/ssh_vm/ubuntu.png"></p><h2 id="install-jupyter-notebook-and-run-it-on-the-virtual-machine">(4) Install jupyter notebook and run it on the virtual machine</h2><p>Firstly, we need to install pip and jupyter notebook on the virtual machine.</p><figure class="highlight routeros"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># switch to root user first</span><br>sudo su<br>apt install python3-pip<br>pip install jupyter notebook<br><span class="hljs-comment"># if it shows error 'E: Package &lt;packagename&gt; has no installation candidate'</span><br><span class="hljs-comment"># run the following commands and then install pip and jupyter notebook</span><br>apt-<span class="hljs-builtin-name">get</span> update<br>apt-<span class="hljs-builtin-name">get</span> upgrade<br></code></pre></td></tr></tbody></table></figure><p>Then we edit the config file of jupyter notebook so that we can remotely log in it on the local browser.</p><figure class="highlight vim"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs vim">jupyter notebook --generate-config<br><span class="hljs-keyword">vim</span> /.../jupyter_notebook_config.<span class="hljs-keyword">py</span><br># after entering the <span class="hljs-keyword">file</span>, press button O <span class="hljs-keyword">to</span> <span class="hljs-built_in">get</span> into <span class="hljs-keyword">edit</span> <span class="hljs-keyword">mode</span><br># <span class="hljs-built_in">add</span> the following three codes into the config <span class="hljs-keyword">file</span><br><span class="hljs-keyword">c</span>.NotebookApp.ip=<span class="hljs-string">'*'</span><br><span class="hljs-keyword">c</span>.NotebookApp.open_browser=False<br><span class="hljs-keyword">c</span>.NotebookApp.port=<span class="hljs-number">8888</span><br># press button esc <span class="hljs-keyword">to</span> turn off <span class="hljs-keyword">edit</span> <span class="hljs-keyword">mode</span>, <span class="hljs-built_in">and</span> <span class="hljs-built_in">type</span> :wq! <span class="hljs-keyword">to</span> save the <span class="hljs-keyword">changes</span> in <span class="hljs-keyword">file</span><br></code></pre></td></tr></tbody></table></figure><p>After everything is set up, we can run jupyter notebook. </p><figure class="highlight ada"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">jupyter notebook <span class="hljs-comment">--allow-root</span><br></code></pre></td></tr></tbody></table></figure><p></p><p><img src="/img/ssh_vm/notebook.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;In this blog, I will introduce how to create a virtual machine on openstack (it&#39;s a cloud platform), and then using SSH to connect the vi</summary>
      
    
    
    
    <category term="Data Engineering" scheme="http://example.com/categories/Data-Engineering/"/>
    
    
    <category term="Virtual Machine" scheme="http://example.com/tags/Virtual-Machine/"/>
    
  </entry>
  
  <entry>
    <title>Install and Control Different Version of Python on Mac</title>
    <link href="http://example.com/2022/01/16/Mac/"/>
    <id>http://example.com/2022/01/16/Mac/</id>
    <published>2022-01-16T20:21:35.000Z</published>
    <updated>2022-04-07T11:35:58.783Z</updated>
    
    <content type="html"><![CDATA[<p>The default version of Python installed on the Mac OS is 2.7, and if we intall Anaconda, the version 3.7 of Python would overwrite the version 2.7 to become the default version of Python in the computer. But now some third-party modules might require the Python version to be 3.8 at least (It's the situation I met when I try to use snscrape, a module for social media data crawler). Under this situation, we need to download and install the right version of Python. When different version of Python exist on the system, we need to arrange them properly to avoid some potential issues.</p><h2 id="download-the-installation-package-from-the-official-website-and-install-it-on-mac">(1) Download the installation package from the official website and install it on Mac</h2><p>Choose macOS 64-bit installer, and doucble click the installer file to install after it's downloaded. Pay attention to the installation path, the default path is /Library/Frameworks/Python.framework/Versions/3.8 <img src="/img/HTCPVOMAC/jieping.png"></p><h2 id="modify-environment-variables">(2) Modify environment variables</h2><p>In order to make the newly installed Python work, we have to add its installation path to environment variables. Run vim ~/.bash_profile on terminal, and then add the following two codes to bash_profile.</p><figure class="highlight elixir"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-comment"># This path is the default installation path, if you install Python to other location, switch this path to your installation path</span><br>export PATH=<span class="hljs-variable">${</span>PATH}<span class="hljs-symbol">:/Library/Frameworks/Python</span>.framework/Versions/<span class="hljs-number">3.8</span>/bin/python3<br><span class="hljs-keyword">alias</span> python=<span class="hljs-string">"/Library/Frameworks/Python.framework/Versions/3.8/bin/python3/python3.8"</span><br></code></pre></td></tr></tbody></table></figure><p>These are to repoint the python command to the newly installed version. After the editing above, run source ~/.bash_profile on terminal to make the execution command above take effect. Then we can run python -V on terminal to check if the current version matches with the newly installed version.</p><h2 id="install-pip">(3) Install pip</h2><p>If we install multiple versions of Python, we also need to install different pip for each version of Python. To install new pip, run the following commands on terminal.</p><figure class="highlight dsconfig"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs dsconfig"><span class="hljs-string">curl</span> <span class="hljs-string">https</span>://<span class="hljs-string">bootstrap</span>.<span class="hljs-string">pypa</span>.<span class="hljs-string">io</span>/<span class="hljs-built_in">get-pip.py</span> -<span class="hljs-string">o</span> <span class="hljs-built_in">get-pip.py</span><br><span class="hljs-comment"># the number after python means your Python version, you can change it to match your Python version</span><br><span class="hljs-string">python3</span>.<span class="hljs-string">8</span> <span class="hljs-built_in">get-pip.py</span><br></code></pre></td></tr></tbody></table></figure><p>Every time pip is installed, it will modify the Python version corresponding to the default pip. Then we should run the following code on terminal to check which pip is corresponding with which Python</p><figure class="highlight nginx"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-comment"># the number behind pip means the serial number of pip, if you have n pip, then run these till pipn -V</span><br><span class="hljs-attribute">pip</span> -V<br>pip2 -V<br>pip3 -V<br></code></pre></td></tr></tbody></table></figure><h2 id="manage-different-version-of-python-to-use-which-pip">(4) Manage different version of Python to use which pip</h2><p>Generally the pip would be corresponding to the Python that is set in the environment variable, if we want to make other Python to use pip, we need to adjust the pip file.</p><figure class="highlight awk"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment">#!/usr/bin/python3.6</span><br><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br>import re<br>import sys<br>from pip._internal import main<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:<br>    sys.argv[<span class="hljs-number">0</span>] = re.sub(<span class="hljs-string">r'(-script\.pyw?|\.exe)?$'</span>, <span class="hljs-string">''</span>, sys.argv[<span class="hljs-number">0</span>])<br>    sys.<span class="hljs-keyword">exit</span>(main())<br></code></pre></td></tr></tbody></table></figure><p>Here the default Python is version 3.6, we can change the first line into other Python installation path, then this pip will be used by the Python we just set. If we want to make the specific version of Python use pip2, pip3, ..., pipn, we just need to set the first line of these pip files to the installation path of the Python we want.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;The default version of Python installed on the Mac OS is 2.7, and if we intall Anaconda, the version 3.7 of Python would overwrite the ve</summary>
      
    
    
    
    <category term="Data Engineering" scheme="http://example.com/categories/Data-Engineering/"/>
    
    
    <category term="Python" scheme="http://example.com/tags/Python/"/>
    
  </entry>
  
</feed>
