

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/IMG.JPG">
  <link rel="icon" href="/img/IMG.JPG">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Cheng Liang">
  <meta name="keywords" content="">
  
    <meta name="description" content="For a small matrix \(A\), we usually compute its eigenvalues through \(det(A-\lambda I)\). But in reality, this computation is too expensive for most matrices. And in many cases, we only need the larg">
<meta property="og:type" content="article">
<meta property="og:title" content="Computation of Eigenvalues and Eigenvectors">
<meta property="og:url" content="http://example.com/2022/07/12/comp-eigen/index.html">
<meta property="og:site_name" content="imchengliang">
<meta property="og:description" content="For a small matrix \(A\), we usually compute its eigenvalues through \(det(A-\lambda I)\). But in reality, this computation is too expensive for most matrices. And in many cases, we only need the larg">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/maxresdefault.jpeg">
<meta property="article:published_time" content="2022-07-12T21:52:20.000Z">
<meta property="article:modified_time" content="2022-07-13T22:06:07.281Z">
<meta property="article:author" content="Cheng Liang">
<meta property="article:tag" content="Linear Algebra">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/maxresdefault.jpeg">
  
  
  <title>Computation of Eigenvalues and Eigenvectors - imchengliang</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/===/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.13","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="imchengliang" type="application/atom+xml">
</head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Liang&#39;s Tech-Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/maxresdefault.jpeg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Computation of Eigenvalues and Eigenvectors">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-07-12 23:52" pubdate>
        July 12, 2022 pm
      </time>
    </span>
  
</div>

<div class="mt-1">
  

  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Computation of Eigenvalues and Eigenvectors</h1>
            
            <div class="markdown-body">
              <p>For a small matrix <span class="math inline">\(A\)</span>, we usually compute its eigenvalues through <span class="math inline">\(det(A-\lambda I)\)</span>. But in reality, this computation is too expensive for most matrices. And in many cases, we only need the largest and/or smallest eigenvalues (or just a few eigenvalues) so it might be overkill to always calculate all eigenvalues.</p>
<p>*Singular value decomposition is linked to eigenvalues and eigenvectors, so methods for eigenvalues and eigenvectors are also methods for SVD.</p>
<h2 id="iterative-methods">Iterative methods</h2>
<p>There are iterative methods for linear equation systems, nonlinear equations, eigenvalues / eigenvectors, optimization. And for eigenvalues/eigenvectors the only option in practice are iterative methods:</p>
<ol type="1">
<li><p>A process that generates a sequence of approximations: <span class="math inline">\(v^{(1)}, v^{(2)}, v^{(3)}, \cdots\)</span></p></li>
<li><p>If the sequence converges the values gets closer and closer to the real value (otherwise it diverges)</p></li>
<li><p>Each step is based on the values in previous step</p></li>
<li><p>Typically an initial value (guess) is needed to start the process</p></li>
<li><p>The iterations are stopped when the values are good enough</p></li>
</ol>
<h2 id="the-power-method">The power method</h2>
<p>The steps of the power method are shown as below:</p>
<ol type="1">
<li><p>Start with <span class="math inline">\(v_{0}=(1, 1, \cdots)^{\top}\)</span>, <span class="math inline">\(v_{t+1}=Av_{t}\)</span> and normalize <span class="math inline">\(v_{t+1}\)</span></p></li>
<li><p>Continue the iteration of (1) until the elements in <span class="math inline">\(v\)</span> don't change anymore</p></li>
<li><p>Normalize <span class="math inline">\(v_u\)</span> we find above with 2-norm to get the eigenvector, and compute the corresponding largest eigenvalue <span class="math inline">\(\lambda = \frac{v_{u}^{\top}(Av_u)}{v_{u}^{\top}v_u}=v_{u}^{\top}Av_u\)</span> (Note, if <span class="math inline">\(v\)</span> is normalized, <span class="math inline">\(v^{\top}v=1\)</span>)</p></li>
</ol>
<p>The mathematics theory behind this method:</p>
<ol type="1">
<li><p>Initial a guess <span class="math inline">\(x^{(0)}\)</span>, if the matrix <span class="math inline">\(A\)</span> is diagonizable, then it has <span class="math inline">\(n\)</span> linearly independent eigenvectors: <span class="math inline">\(v_{1}, v_{2}, \cdots, v_{n}\)</span></p></li>
<li><p><span class="math inline">\(x^{(0)}\)</span> can be expressed as <span class="math inline">\(x^{(0)}=c_{1}v_{1}+c_{2}v_{2}+ \cdots +c_{n}v_{n}\)</span></p></li>
<li><p>Multiply by <span class="math inline">\(A\Rightarrow x^{(0)} = A x^{(0)} =A\left[c_{1} v_{1}+c_{2} v_{2}+\cdots+c_{n} V_{n}\right]\)</span><br>
<span class="math inline">\(\quad \quad \quad \quad \quad \quad \quad \quad \enspace=c_{1} A v_{1}+c_{2} A v_{2}+\cdots+c_{n} A v_{n}\)</span><br>
<span class="math inline">\(\quad \quad \quad \quad \quad \quad \quad \quad \enspace=c_{1} \lambda_{1} v_{1}+c_{2} \lambda_{2} v_{2}+\cdots+c_{n} \lambda _{n} V_{n}\)</span><br>
Normalize the new <span class="math inline">\(x^{(0)} = \frac{x^{(0)}}{\parallel x^{(0)} \parallel _2}\)</span></p></li>
<li><p>Multiply by <span class="math inline">\(A\)</span> again <span class="math inline">\(\Rightarrow x^{(0)} = AA x^{(0)} =c_{1} \lambda_{1} A v_{1}+c_{2} \lambda_{2} A v_{2}+\cdots+c_{n} \lambda _{n} A V_{n}\)</span><br>
<span class="math inline">\(\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad =c_{1} {\lambda_{1}}^{2} v_{1}+c_{2} {\lambda_{2}}^{2} v_{2}+\cdots+c_{n} {\lambda _{n}}^{2} V_{n}\)</span><br>
<span class="math inline">\(\quad \quad \quad \quad \quad \quad \quad \quad \Rightarrow x^{(0)} = A^{k}x^{(0)}=c_{1} {\lambda_{1}}^{k} v_{1}+c_{2} {\lambda_{2}}^{k} v_{2}+\cdots+c_{n} {\lambda _{n}}^{k} V_{n}\)</span><br>
Normalize the new $x^{(0)} again</p></li>
<li><p>Assume <span class="math inline">\(\lambda _{1}\)</span> is the largest in magnitude<br>
<span class="math inline">\(A^{k}x^{(0)}={\lambda_{1}}^{k}(c_{1}v_{1}+c_{2}(\frac{\lambda _{2}}{\lambda _{1}})^{k}v_{2}+ \cdots + c_{n}(\frac{\lambda _{n}}{\lambda _{1}})^{k} v_{n})\)</span><br>
<span class="math inline">\(\qquad \quad \approx{\lambda _{1}}^{k}c_{1}v_{1}\)</span>, <span class="math inline">\(\quad\)</span> when <span class="math inline">\(k\rightarrow \infty\)</span><br>
Then we get a multiple of the dominant eigenvector.</p></li>
</ol>
<p>The convegence rate of the power method depends on the ratio of <span class="math inline">\(\lambda _{n}\)</span> and <span class="math inline">\(\lambda _{1}\)</span> (<span class="math inline">\(n=2,3,\cdots\)</span>). The convegence would be faster as <span class="math inline">\(\frac{\lambda _{n}}{\lambda _{1}}\)</span> to be smaller.</p>
<p>To find the smallest eigenvalue and corresponding eigenvector, we can perform the same iteration on <span class="math inline">\(A^{-1}\)</span>, called inverse power iteration.</p>
<p>By using shift <span class="math inline">\(\mu\)</span> and use the power method on the eigenvalue problem, we can find other eigenvalues than the largest: <span class="math inline">\((A-\mu I)v = (\lambda - \mu)v\)</span> In the case of <span class="math inline">\(\lambda _{1} = \mu\)</span>, the smallest eigenvalue becomes the largest in the shifted problem, and the Power method will find the smallest eigenvalue (to the initial problem). In principle we can find all eigenvalues by using shifts, but others methods are used in practice.</p>
<h2 id="the-qr-method">The QR-method</h2>
<p>The QR-method (or QR-iteration) is a method that can be used to find all eigenvalues at the same time. The idea is to in some way find transformations similar to <span class="math inline">\(A = QΛQ^{\top}\)</span> and then read off the eigenvalues on the diagonal of <span class="math inline">\(Λ\)</span>.</p>
<p>For the matrices that are not diagonalizable, we use a different similarity transform. If <span class="math inline">\(A = VBV^{\top}\)</span> for some nonsingular matrix <span class="math inline">\(V\)</span>, then <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are similar, so they have the same eigenvalues. If <span class="math inline">\(B\)</span> happens to be triangular, then we can read off the eigenvalues of <span class="math inline">\(A\)</span> from the diagonal of <span class="math inline">\(B\)</span>.</p>
<p>Schur decomposition (Schur form): If matrix <span class="math inline">\(A\)</span> real with real eigenvalues then there exist orthogonal matrix <span class="math inline">\(Q\)</span> such that <span class="math inline">\(A = QSQ^{-1} = QSQ^{\top}\)</span>, <span class="math inline">\(S\)</span> real and upper triangular. <span class="math inline">\(A\)</span> and <span class="math inline">\(S\)</span> are similar, so <span class="math inline">\(A\)</span>’s eigenvalues on the diagonal of <span class="math inline">\(S\)</span>. If <span class="math inline">\(A\)</span> real with complex eigenvalues then there exist orthogonal matrix <span class="math inline">\(Q\)</span> such that <span class="math inline">\(𝐴 = Q \tilde{S} Q^{-1} = Q \tilde{S} Q^{T}\)</span>, <span class="math inline">\(\tilde{S}\)</span> is quasi upper triangular.</p>
<ol type="1">
<li><p>Start with <span class="math inline">\(A\)</span> doing QR-factorization: <span class="math inline">\(A=QR\)</span>, then <span class="math inline">\(A^{(1)}=RQ=Q^{\top}AQ\)</span></p></li>
<li><p>Next step: <span class="math inline">\(A^{(1)}=Q^{(1)}R\)</span>, form <span class="math inline">\(A^{(2)}=RQ^{(1)}={Q^{(1)}}^{\top}A^{(1)}Q^{(1)}\)</span>, and so forth</p></li>
</ol>
<p>If we look at <span class="math inline">\(A^{(3)}={Q^{(2)}}^{\top}{Q^{(1)}}^{\top}Q^{\top}AQQ^{(1)}Q^{(2)}\)</span>, <span class="math inline">\(\tilde{Q}\)</span> will converge to <span class="math inline">\(V\)</span> in the schur form contains eigenvectors, <span class="math inline">\(A^{(k)}\)</span> will converge to <span class="math inline">\(S\)</span> in schur form (triangular and eigenvalues on diagonal).</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Mathematics/">Mathematics</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Linear-Algebra/">Linear Algebra</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    All articles in this blog adopt the <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 agreement</a> except for special statements. Please indicate the source for reprinting!
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/08/13/linear-regression/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Linear Regression</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/07/11/PCA/">
                        <span class="hidden-mobile">Principal Component Analysis (PCA)</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
